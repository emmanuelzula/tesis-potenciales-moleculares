{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchMD-NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacion de modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml  # Para cargar configuraciones desde archivos YAML\n",
    "import argparse  # Para manejar argumentos de línea de comandos\n",
    "import numpy as np  # Para operaciones matemáticas con arrays\n",
    "import torch  # Biblioteca principal para deep learning con PyTorch\n",
    "from os.path import dirname, join, exists  # Para trabajar con rutas de archivos y directorios\n",
    "from pytorch_lightning.utilities import rank_zero_warn  # Para mostrar advertencias específicas de PyTorch Lightning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion de masas atomicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# Atomic masses are based on:\n",
    "#\n",
    "#   Meija, J., Coplen, T., Berglund, M., et al. (2016). Atomic weights of\n",
    "#   the elements 2013 (IUPAC Technical Report). Pure and Applied Chemistry,\n",
    "#   88(3), pp. 265-291. Retrieved 30 Nov. 2016,\n",
    "#   from doi:10.1515/pac-2015-0305\n",
    "#\n",
    "# Standard atomic weights are taken from Table 1: \"Standard atomic weights\n",
    "# 2013\", with the uncertainties ignored.\n",
    "# For hydrogen, helium, boron, carbon, nitrogen, oxygen, magnesium, silicon,\n",
    "# sulfur, chlorine, bromine and thallium, where the weights are given as a\n",
    "# range the \"conventional\" weights are taken from Table 3 and the ranges are\n",
    "# given in the comments.\n",
    "# The mass of the most stable isotope (in Table 4) is used for elements\n",
    "# where there the element has no stable isotopes (to avoid NaNs): Tc, Pm,\n",
    "# Po, At, Rn, Fr, Ra, Ac, everything after N\n",
    "\n",
    "# Definición de masas atómicas de varios elementos.\n",
    "atomic_masses = np.array([\n",
    "    1.0, 1.008, 4.002602, 6.94, 9.0121831,\n",
    "    10.81, 12.011, 14.007, 15.999, 18.998403163,\n",
    "    20.1797, 22.98976928, 24.305, 26.9815385, 28.085,\n",
    "    30.973761998, 32.06, 35.45, 39.948, 39.0983,\n",
    "    40.078, 44.955908, 47.867, 50.9415, 51.9961,\n",
    "    54.938044, 55.845, 58.933194, 58.6934, 63.546,\n",
    "    65.38, 69.723, 72.63, 74.921595, 78.971,\n",
    "    79.904, 83.798, 85.4678, 87.62, 88.90584,\n",
    "    91.224, 92.90637, 95.95, 97.90721, 101.07,\n",
    "    102.9055, 106.42, 107.8682, 112.414, 114.818,\n",
    "    118.71, 121.76, 127.6, 126.90447, 131.293,\n",
    "    132.90545196, 137.327, 138.90547, 140.116, 140.90766,\n",
    "    144.242, 144.91276, 150.36, 151.964, 157.25,\n",
    "    158.92535, 162.5, 164.93033, 167.259, 168.93422,\n",
    "    173.054, 174.9668, 178.49, 180.94788, 183.84,\n",
    "    186.207, 190.23, 192.217, 195.084, 196.966569,\n",
    "    200.592, 204.38, 207.2, 208.9804, 208.98243,\n",
    "    209.98715, 222.01758, 223.01974, 226.02541, 227.02775,\n",
    "    232.0377, 231.03588, 238.02891, 237.04817, 244.06421,\n",
    "    243.06138, 247.07035, 247.07031, 251.07959, 252.083,\n",
    "    257.09511, 258.09843, 259.101, 262.11, 267.122,\n",
    "    268.126, 271.134, 270.133, 269.1338, 278.156,\n",
    "    281.165, 281.166, 285.177, 286.182, 289.19,\n",
    "    289.194, 293.204, 293.208, 294.214,\n",
    "])\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que divide un conjunto de datos en conjuntos de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dset_len, train_size, val_size, test_size, seed, order=None):\n",
    "    # Asegura que solo uno de train_size, val_size o test_size sea None.\n",
    "    assert (train_size is None) + (val_size is None) + (\n",
    "        test_size is None\n",
    "    ) <= 1, \"Only one of train_size, val_size, test_size is allowed to be None.\"\n",
    "    \n",
    "    # Comprueba si train_size, val_size, test_size son flotantes.\n",
    "    is_float = (\n",
    "        isinstance(train_size, float),\n",
    "        isinstance(val_size, float),\n",
    "        isinstance(test_size, float),\n",
    "    )\n",
    "\n",
    "    # Redondea las fracciones a números enteros si son flotantes.\n",
    "    train_size = round(dset_len * train_size) if is_float[0] else train_size\n",
    "    val_size = round(dset_len * val_size) if is_float[1] else val_size\n",
    "    test_size = round(dset_len * test_size) if is_float[2] else test_size\n",
    "\n",
    "    # Calcula las dimensiones faltantes si alguna de ellas es None.\n",
    "    if train_size is None:\n",
    "        train_size = dset_len - val_size - test_size\n",
    "    elif val_size is None:\n",
    "        val_size = dset_len - train_size - test_size\n",
    "    elif test_size is None:\n",
    "        test_size = dset_len - train_size - val_size\n",
    "\n",
    "    # Ajusta los tamaños si la suma supera el tamaño del dataset.\n",
    "    if train_size + val_size + test_size > dset_len:\n",
    "        if is_float[2]:\n",
    "            test_size -= 1\n",
    "        elif is_float[1]:\n",
    "            val_size -= 1\n",
    "        elif is_float[0]:\n",
    "            train_size -= 1\n",
    "\n",
    "    # Asegura que los tamaños resultantes sean no negativos.\n",
    "    assert train_size >= 0 and val_size >= 0 and test_size >= 0, (\n",
    "        f\"One of training ({train_size}), validation ({val_size}) or \"\n",
    "        f\"testing ({test_size}) splits ended up with a negative size.\"\n",
    "    )\n",
    "\n",
    "    # Calcula el tamaño total de las divisiones y compara con el tamaño del dataset.\n",
    "    total = train_size + val_size + test_size\n",
    "    assert dset_len >= total, (\n",
    "        f\"The dataset ({dset_len}) is smaller than the \"\n",
    "        f\"combined split sizes ({total}).\"\n",
    "    )\n",
    "    \n",
    "    # Advierte si se excluyeron muestras del dataset.\n",
    "    if total < dset_len:\n",
    "        rank_zero_warn(f\"{dset_len - total} samples were excluded from the dataset\")\n",
    "\n",
    "    # Genera índices de forma secuencial para el dataset.\n",
    "    idxs = np.arange(dset_len, dtype=int)\n",
    "\n",
    "    # Permuta los índices si se proporciona una semilla y un orden específico.\n",
    "    if order is None:\n",
    "        idxs = np.random.default_rng(seed).permutation(idxs)\n",
    "\n",
    "    # Divide los índices en conjuntos de entrenamiento, validación y prueba.\n",
    "    idx_train = idxs[:train_size]\n",
    "    idx_val = idxs[train_size : train_size + val_size]\n",
    "    idx_test = idxs[train_size + val_size : total]\n",
    "\n",
    "    # Reorganiza los índices según el orden proporcionado si existe.\n",
    "    if order is not None:\n",
    "        idx_train = [order[i] for i in idx_train]\n",
    "        idx_val = [order[i] for i in idx_val]\n",
    "        idx_test = [order[i] for i in idx_test]\n",
    "\n",
    "    # Devuelve los índices en forma de arrays NumPy.\n",
    "    return np.array(idx_train), np.array(idx_val), np.array(idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera divisiones (splits) de un conjunto de datos en conjuntos de entrenamiento, validación y prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splits(\n",
    "    dataset_len,\n",
    "    train_size,\n",
    "    val_size,\n",
    "    test_size,\n",
    "    seed,\n",
    "    filename=None,\n",
    "    splits=None,\n",
    "    order=None,\n",
    "):\n",
    "    # Comprueba si ya existen divisiones cargadas desde un archivo.\n",
    "    if splits is not None:\n",
    "        splits = np.load(splits)\n",
    "        idx_train = splits[\"idx_train\"]\n",
    "        idx_val = splits[\"idx_val\"]\n",
    "        idx_test = splits[\"idx_test\"]\n",
    "    else:\n",
    "        # Si no hay divisiones preexistentes, llama a la función train_val_test_split\n",
    "        # para generar nuevas divisiones de los datos.\n",
    "        idx_train, idx_val, idx_test = train_val_test_split(\n",
    "            dataset_len, train_size, val_size, test_size, seed, order\n",
    "        )\n",
    "\n",
    "    # Si se proporciona un nombre de archivo, guarda las divisiones en un archivo NPZ.\n",
    "    if filename is not None:\n",
    "        np.savez(filename, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test)\n",
    "\n",
    "    # Devuelve los índices de las divisiones como tensores de PyTorch.\n",
    "    return (\n",
    "        torch.from_numpy(idx_train),\n",
    "        torch.from_numpy(idx_val),\n",
    "        torch.from_numpy(idx_test),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase llamada LoadFromFile que se utiliza como acción personalizada en la creación de argumentos de línea de comandos utilizando el módulo argparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadFromFile(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        # Comprueba si el archivo proporcionado es un archivo YAML (con extensión .yaml o .yml).\n",
    "        if values.name.endswith(\"yaml\") or values.name.endswith(\"yml\"):\n",
    "            # Abre el archivo en modo lectura.\n",
    "            with values as f:\n",
    "                # Carga el contenido del archivo YAML en un diccionario utilizando el cargador FullLoader de PyYAML.\n",
    "                config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "            \n",
    "            # Comprueba si las claves del archivo YAML son argumentos válidos.\n",
    "            for key in config.keys():\n",
    "                if key not in namespace:\n",
    "                    raise ValueError(f\"Unknown argument in config file: {key}\")\n",
    "            \n",
    "            # Si la clave \"load_model\" está presente en el archivo YAML y se especificó el argumento --load_model\n",
    "            # en la línea de comandos, se muestra una advertencia y se ignora la clave \"load_model\" del archivo YAML.\n",
    "            if (\n",
    "                \"load_model\" in config\n",
    "                and namespace.load_model is not None\n",
    "                and config[\"load_model\"] != namespace.load_model\n",
    "            ):\n",
    "                rank_zero_warn(\n",
    "                    f\"The load model argument was specified as a command line argument \"\n",
    "                    f\"({namespace.load_model}) and in the config file ({config['load_model']}). \"\n",
    "                    f\"Ignoring 'load_model' from the config file and loading {namespace.load_model}.\"\n",
    "                )\n",
    "                del config[\"load_model\"]\n",
    "            \n",
    "            # Actualiza el espacio de nombres (namespace) con las claves y valores del archivo YAML.\n",
    "            namespace.__dict__.update(config)\n",
    "        else:\n",
    "            # Si el archivo no tiene extensión .yaml o .yml, se genera un error.\n",
    "            raise ValueError(\"Configuration file must end with yaml or yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase que se utiliza como acción personalizada en la creación de argumentos de línea de comandos utilizando el módulo argparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadFromCheckpoint(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        # Construye la ruta al archivo 'hparams.yaml' a partir de la ruta del archivo del checkpoint.\n",
    "        hparams_path = join(dirname(values), \"hparams.yaml\")\n",
    "        \n",
    "        # Verifica si el archivo 'hparams.yaml' existe en la ubicación especificada.\n",
    "        if not exists(hparams_path):\n",
    "            # Si no existe, imprime un mensaje indicando que no se pudo encontrar el archivo y se confía en los argumentos de la línea de comandos.\n",
    "            print(\n",
    "                \"Failed to locate the checkpoint's hparams.yaml file. Relying on command line args.\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # Si el archivo 'hparams.yaml' existe, lo abre en modo lectura.\n",
    "        with open(hparams_path, \"r\") as f:\n",
    "            # Carga el contenido del archivo YAML en un diccionario utilizando el cargador FullLoader de PyYAML.\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "        # Comprueba si las claves del archivo YAML son argumentos válidos, excepto la clave \"prior_args\".\n",
    "        for key in config.keys():\n",
    "            if key not in namespace and key != \"prior_args\":\n",
    "                raise ValueError(f\"Unknown argument in the model checkpoint: {key}\")\n",
    "        \n",
    "        # Actualiza el espacio de nombres (namespace) con las claves y valores del archivo YAML.\n",
    "        # También agrega una clave 'load_model' al espacio de nombres con el valor del archivo de checkpoint.\n",
    "        namespace.__dict__.update(config)\n",
    "        namespace.__dict__.update(load_model=values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una función que se utiliza para guardar argumentos (posiblemente excluyendo algunos) en un archivo de configuración YAML o JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_argparse(args, filename, exclude=None):\n",
    "    import json  # Importa el módulo json\n",
    "\n",
    "    # Comprueba si el nombre de archivo tiene una extensión YAML o YML.\n",
    "    if filename.endswith(\"yaml\") or filename.endswith(\"yml\"):\n",
    "        # Si se proporciona un valor para excluir argumentos, conviértelo en una lista.\n",
    "        if isinstance(exclude, str):\n",
    "            exclude = [exclude]\n",
    "\n",
    "        # Copia los argumentos en un nuevo diccionario.\n",
    "        args = args.__dict__.copy()\n",
    "\n",
    "        # Elimina los argumentos especificados en la lista de exclusión.\n",
    "        for exl in exclude:\n",
    "            del args[exl]\n",
    "\n",
    "        # Obtén el valor del argumento \"dataset_arg\".\n",
    "        ds_arg = args.get(\"dataset_arg\")\n",
    "\n",
    "        # Si \"dataset_arg\" existe y es una cadena, intenta cargarlo como un objeto JSON.\n",
    "        if ds_arg is not None and isinstance(ds_arg, str):\n",
    "            args[\"dataset_arg\"] = json.loads(args[\"dataset_arg\"])\n",
    "\n",
    "        # Guarda los argumentos en el archivo de configuración YAML.\n",
    "        yaml.dump(args, open(filename, \"w\"))\n",
    "    else:\n",
    "        # Si el archivo no tiene una extensión YAML o YML, genera un error.\n",
    "        raise ValueError(\"Configuration file should end with yaml or yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una función que se utiliza para convertir una cadena de texto en un número, ya sea un número entero (int) o un número de punto flotante (float). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number(text):\n",
    "    # Comprueba si el texto es None o \"None\" y devuelve None en ese caso.\n",
    "    if text is None or text == \"None\":\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Intenta convertir el texto en un número entero.\n",
    "        num_int = int(text)\n",
    "    except ValueError:\n",
    "        # Si la conversión a entero falla, establece num_int como None.\n",
    "        num_int = None\n",
    "    \n",
    "    # Convierte el texto en un número de punto flotante.\n",
    "    num_float = float(text)\n",
    "\n",
    "    # Compara el número entero y el número de punto flotante.\n",
    "    # Si son iguales, devuelve el número entero, de lo contrario, devuelve el número de punto flotante.\n",
    "    if num_int == num_float:\n",
    "        return num_int\n",
    "    return num_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase llamada MissingEnergyException que hereda de la clase base Exception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingEnergyException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones recurrentes del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacion de modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Importa el módulo math para operaciones matemáticas.\n",
    "from typing import Optional, Tuple  # Importa tipos y anotaciones de tipos.\n",
    "import torch  # Importa PyTorch, una biblioteca para aprendizaje profundo.\n",
    "from torch import Tensor  # Importa el tipo de datos Tensor de PyTorch.\n",
    "from torch import nn  # Importa el módulo de redes neuronales de PyTorch.\n",
    "import torch.nn.functional as F  # Importa funciones de activación y capas de PyTorch.\n",
    "from torch_geometric.nn import MessagePassing  # Importa MessagePassing de PyTorch Geometric.\n",
    "from torch_cluster import radius_graph  # Importa radius_graph de Torch Cluster.\n",
    "import warnings  # Importa el módulo de advertencias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una función que se utiliza para visualizar una base específica de funciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_basis(basis_type, num_rbf=50, cutoff_lower=0, cutoff_upper=5):\n",
    "    \"\"\"\n",
    "    Function for quickly visualizing a specific basis. This is useful for inspecting\n",
    "    the distance coverage of basis functions for non-default lower and upper cutoffs.\n",
    "\n",
    "    Args:\n",
    "        basis_type (str): Specifies the type of basis functions used. Can be one of\n",
    "            ['gauss', 'expnorm']\n",
    "        num_rbf (int, optional): The number of basis functions.\n",
    "            (default: :obj:`50`)\n",
    "        cutoff_lower (float, optional): The lower cutoff of the basis.\n",
    "            (default: :obj:`0`)\n",
    "        cutoff_upper (float, optional): The upper cutoff of the basis.\n",
    "            (default: :obj:`5`)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt  # Importa el módulo Matplotlib para graficar.\n",
    "\n",
    "    # Genera una serie de distancias para visualización.\n",
    "    distances = torch.linspace(cutoff_lower - 1, cutoff_upper + 1, 1000)\n",
    "    \n",
    "    # Define los parámetros de la base de funciones.\n",
    "    basis_kwargs = {\n",
    "        \"num_rbf\": num_rbf,\n",
    "        \"cutoff_lower\": cutoff_lower,\n",
    "        \"cutoff_upper\": cutoff_upper,\n",
    "    }\n",
    "    \n",
    "    # Crea una instancia de la base de funciones especificada.\n",
    "    basis_expansion = rbf_class_mapping[basis_type](**basis_kwargs)\n",
    "    \n",
    "    # Calcula las distancias expandidas utilizando la base de funciones.\n",
    "    expanded_distances = basis_expansion(distances)\n",
    "\n",
    "    # Grafica las distancias expandidas para cada función base.\n",
    "    for i in range(expanded_distances.shape[-1]):\n",
    "        plt.plot(distances.numpy(), expanded_distances[:, i].detach().numpy())\n",
    "    \n",
    "    # Muestra la gráfica.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase que hereda de la clase MessagePassing de PyTorch Geometric. Esta clase se utiliza para implementar la arquitectura de aprendizaje profundo de Equivariant Transformer (ET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborEmbedding(MessagePassing):\n",
    "    def __init__(self, hidden_channels, num_rbf, cutoff_lower, cutoff_upper, max_z=100, dtype=torch.float32):\n",
    "        \"\"\"\n",
    "        The ET architecture assigns two learned vectors to each atom type zi. \n",
    "        One is used to encode information specific to an atom, the other (this class) \n",
    "        takes the role of a neighborhood embedding. The neighborhood embedding, \n",
    "        which is an embedding of the types of neighboring atoms, is multiplied by a \n",
    "        distance filter. This embedding allows the network to store information about \n",
    "        the interaction of atom pairs.\n",
    "        \n",
    "        See eq. 3 in https://arxiv.org/pdf/2202.02541.pdf for more details.\n",
    "        \"\"\"\n",
    "        super(NeighborEmbedding, self).__init__(aggr=\"add\")\n",
    "        \n",
    "        # Inicializa la capa de embedding para representar el tipo de átomo.\n",
    "        self.embedding = nn.Embedding(max_z, hidden_channels, dtype=dtype)\n",
    "        \n",
    "        # Inicializa la capa de proyección para las distancias.\n",
    "        self.distance_proj = nn.Linear(num_rbf, hidden_channels, dtype=dtype)\n",
    "        \n",
    "        # Inicializa la capa de combinación de características.\n",
    "        self.combine = nn.Linear(hidden_channels * 2, hidden_channels, dtype=dtype)\n",
    "        \n",
    "        # Inicializa la función de corte basada en el coseno.\n",
    "        self.cutoff = CosineCutoff(cutoff_lower, cutoff_upper)\n",
    "\n",
    "        # Reinicia los parámetros de la red.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Reinicia los parámetros de embedding.\n",
    "        self.embedding.reset_parameters()\n",
    "        \n",
    "        # Inicializa los pesos de las capas con xavier_uniform.\n",
    "        nn.init.xavier_uniform_(self.distance_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.combine.weight)\n",
    "        \n",
    "        # Inicializa los sesgos de la capa de proyección de distancias.\n",
    "        self.distance_proj.bias.data.fill_(0)\n",
    "        \n",
    "        # Inicializa los sesgos de la capa de combinación de características.\n",
    "        self.combine.bias.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        edge_weight: Tensor,\n",
    "        edge_attr: Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z (Tensor): Atomic numbers of shape :obj:`[num_nodes]`\n",
    "            x (Tensor): Node feature matrix (atom positions) of shape :obj:`[num_nodes, 3]`\n",
    "            edge_index (Tensor): Graph connectivity (list of neighbor pairs) with shape :obj:`[2, num_edges]`\n",
    "            edge_weight (Tensor): Edge weight vector of shape :obj:`[num_edges]`\n",
    "            edge_attr (Tensor): Edge attribute matrix of shape :obj:`[num_edges, 3]`\n",
    "        Returns:\n",
    "            x_neighbors (Tensor): The embedding of the neighbors of each atom of shape :obj:`[num_nodes, hidden_channels]`\n",
    "        \"\"\"\n",
    "        # Elimina auto conexiones (self loops).\n",
    "        mask = edge_index[0] != edge_index[1]\n",
    "        if not mask.all():\n",
    "            edge_index = edge_index[:, mask]\n",
    "            edge_weight = edge_weight[mask]\n",
    "            edge_attr = edge_attr[mask]\n",
    "\n",
    "        # Calcula el filtro de corte (cutoff) basado en el coseno.\n",
    "        C = self.cutoff(edge_weight)\n",
    "        \n",
    "        # Proyecta las distancias en un espacio de características.\n",
    "        W = self.distance_proj(edge_attr) * C.view(-1, 1)\n",
    "\n",
    "        # Obtiene la representación de embedding para el tipo de átomo.\n",
    "        x_neighbors = self.embedding(z)\n",
    "        \n",
    "        # Realiza la propagación de mensajes entre nodos.\n",
    "        x_neighbors = self.propagate(edge_index, x=x_neighbors, W=W, size=None)\n",
    "        \n",
    "        # Combina las características de los nodos con las de los vecinos.\n",
    "        x_neighbors = self.combine(torch.cat([x, x_neighbors], dim=1))\n",
    "        \n",
    "        return x_neighbors\n",
    "\n",
    "    def message(self, x_j, W):\n",
    "        return x_j * W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase que es una capa personalizada en PyTorch. Esta capa se utiliza para calcular la lista de vecinos para un valor de corte (cutoff) dado en una estructura de átomos tridimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDistance(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff_lower=0.0,\n",
    "        cutoff_upper=5.0,\n",
    "        max_num_pairs=-32,\n",
    "        return_vecs=False,\n",
    "        loop=False,\n",
    "        strategy=\"brute\",\n",
    "        include_transpose=True,\n",
    "        resize_to_fit=True,\n",
    "        check_errors=True,\n",
    "        box=None,\n",
    "    ):\n",
    "        super(OptimizedDistance, self).__init__()\n",
    "        \"\"\" Compute the neighbor list for a given cutoff.\n",
    "        This operation can be placed inside a CUDA graph in some cases.\n",
    "        In particular, resize_to_fit and check_errors must be False.\n",
    "        Note that this module returns neighbors such that distance(i,j) >= cutoff_lower and distance(i,j) < cutoff_upper.\n",
    "        This function optionally supports periodic boundary conditions with\n",
    "        arbitrary triclinic boxes.  The box vectors `a`, `b`, and `c` must satisfy\n",
    "        certain requirements:\n",
    "\n",
    "        `a[1] = a[2] = b[2] = 0`\n",
    "        `a[0] >= 2*cutoff, b[1] >= 2*cutoff, c[2] >= 2*cutoff`\n",
    "        `a[0] >= 2*b[0]`\n",
    "        `a[0] >= 2*c[0]`\n",
    "        `b[1] >= 2*c[1]`\n",
    "\n",
    "        These requirements correspond to a particular rotation of the system and\n",
    "        reduced form of the vectors, as well as the requirement that the cutoff be\n",
    "        no larger than half the box width.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cutoff_lower : float\n",
    "            Lower cutoff for the neighbor list.\n",
    "        cutoff_upper : float\n",
    "            Upper cutoff for the neighbor list.\n",
    "        max_num_pairs : int\n",
    "            Maximum number of pairs to store, if the number of pairs found is less than this, the list is padded with (-1,-1) pairs up to max_num_pairs unless resize_to_fit is True, in which case the list is resized to the actual number of pairs found.\n",
    "            If the number of pairs found is larger than this, the pairs are randomly sampled. When check_errors is True, an exception is raised in this case.\n",
    "            If negative, it is interpreted as (minus) the maximum number of neighbors per atom.\n",
    "        strategy : str\n",
    "            Strategy to use for computing the neighbor list. Can be one of\n",
    "            [\"shared\", \"brute\", \"cell\"].\n",
    "            Shared: An O(N^2) algorithm that leverages CUDA shared memory, best for large number of particles.\n",
    "            Brute: A brute force O(N^2) algorithm, best for small number of particles.\n",
    "            Cell:  A cell list algorithm, best for large number of particles, low cutoffs and low batch size.\n",
    "        box : torch.Tensor, optional\n",
    "            The vectors defining the periodic box.  This must have shape `(3, 3)`,\n",
    "            where `box_vectors[0] = a`, `box_vectors[1] = b`, and `box_vectors[2] = c`.\n",
    "            If this is omitted, periodic boundary conditions are not applied.\n",
    "        loop : bool, optional\n",
    "            Whether to include self-interactions.\n",
    "            Default: False\n",
    "        include_transpose : bool, optional\n",
    "            Whether to include the transpose of the neighbor list.\n",
    "            Default: True\n",
    "        resize_to_fit : bool, optional\n",
    "            Whether to resize the neighbor list to the actual number of pairs found. When False, the list is padded with (-1,-1) pairs up to max_num_pairs\n",
    "            Default: True\n",
    "            If this is True the operation is not CUDA graph compatible.\n",
    "        check_errors : bool, optional\n",
    "            Whether to check for too many pairs. If this is True the operation is not CUDA graph compatible.\n",
    "            Default: True\n",
    "        return_vecs : bool, optional\n",
    "            Whether to return the distance vectors.\n",
    "            Default: False\n",
    "        \"\"\"\n",
    "        # Inicializa los parámetros relacionados con la lista de vecinos.\n",
    "        self.cutoff_upper = cutoff_upper\n",
    "        self.cutoff_lower = cutoff_lower\n",
    "        self.max_num_pairs = max_num_pairs\n",
    "        self.strategy = strategy\n",
    "        self.box: Optional[Tensor] = box\n",
    "        self.loop = loop\n",
    "        self.return_vecs = return_vecs\n",
    "        self.include_transpose = include_transpose\n",
    "        self.resize_to_fit = resize_to_fit\n",
    "        self.use_periodic = True\n",
    "        \n",
    "        # Configura el box periódico si se proporciona, de lo contrario, se asume que no hay condiciones periódicas.\n",
    "        if self.box is None:\n",
    "            self.use_periodic = False\n",
    "            self.box = torch.empty((0, 0))\n",
    "            if self.strategy == \"cell\":\n",
    "                # Establece un box predeterminado para la estrategia \"cell\" (lista de celdas).\n",
    "                lbox = cutoff_upper * 3.0\n",
    "                self.box = torch.tensor([[lbox, 0, 0], [0, lbox, 0], [0, 0, lbox]])\n",
    "        \n",
    "        # Mueve el box a la memoria CPU, ya que todas las estrategias esperan que esté en la CPU.\n",
    "        self.box = self.box.cpu()\n",
    "        \n",
    "        # Configura si se deben verificar errores relacionados con el número de pares de vecinos.\n",
    "        self.check_errors = check_errors\n",
    "        \n",
    "        # Importa la función del kernel que se utiliza para calcular la lista de vecinos.\n",
    "        from torchmdnet.neighbors import get_neighbor_pairs_kernel\n",
    "        self.kernel = get_neighbor_pairs_kernel\n",
    "\n",
    "    def forward(\n",
    "        self, pos: Tensor, batch: Optional[Tensor] = None\n",
    "    ) -> Tuple[Tensor, Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Compute the neighbor list for a given cutoff.\n",
    "        Parameters\n",
    "        ----------\n",
    "        pos : torch.Tensor\n",
    "            shape (N, 3)\n",
    "        batch : torch.Tensor or None\n",
    "            shape (N,)\n",
    "        Returns\n",
    "        -------\n",
    "        edge_index : torch.Tensor\n",
    "          List of neighbors for each atom in the batch.\n",
    "        shape (2, num_found_pairs or max_num_pairs)\n",
    "        edge_weight : torch.Tensor\n",
    "            List of distances for each atom in the batch.\n",
    "        shape (num_found_pairs or max_num_pairs,)\n",
    "        edge_vec : torch.Tensor\n",
    "            List of distance vectors for each atom in the batch.\n",
    "        shape (num_found_pairs or max_num_pairs, 3)\n",
    "\n",
    "        If resize_to_fit is True, the tensors will be trimmed to the actual number of pairs found.\n",
    "        otherwise the tensors will have size max_num_pairs, with neighbor pairs (-1, -1) at the end.\n",
    "\n",
    "        \"\"\"\n",
    "        # Asegura que el box esté en el mismo tipo de datos que las posiciones.\n",
    "        self.box = self.box.to(pos.dtype)\n",
    "\n",
    "        # Calcula el número máximo de pares de vecinos a almacenar.\n",
    "        max_pairs = self.max_num_pairs\n",
    "        if self.max_num_pairs < 0:\n",
    "            max_pairs = -self.max_num_pairs * pos.shape[0]\n",
    "\n",
    "        # Si batch es None, se asigna a todas las partículas al mismo lote (batch 0).\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(pos.shape[0], dtype=torch.long, device=pos.device)\n",
    "\n",
    "        # Llama al kernel para calcular la lista de vecinos.\n",
    "        edge_index, edge_vec, edge_weight, num_pairs = self.kernel(\n",
    "            strategy=self.strategy,\n",
    "            positions=pos,\n",
    "            batch=batch,\n",
    "            max_num_pairs=max_pairs,\n",
    "            cutoff_lower=self.cutoff_lower,\n",
    "            cutoff_upper=self.cutoff_upper,\n",
    "            loop=self.loop,\n",
    "            include_transpose=self.include_transpose,\n",
    "            box_vectors=self.box,\n",
    "            use_periodic=self.use_periodic,\n",
    "        )\n",
    "\n",
    "        # Verifica errores si se habilita la comprobación de errores.\n",
    "        if self.check_errors:\n",
    "            if num_pairs[0] > max_pairs:\n",
    "                raise RuntimeError(\n",
    "                    \"Encontrado num_pairs({}) > max_num_pairs({})\".format(\n",
    "                        num_pairs[0], max_pairs\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Convierte edge_index a tipo de datos de enteros.\n",
    "        edge_index = edge_index.to(torch.long)\n",
    "\n",
    "        # Elimina pares de vecinos (-1, -1) si resize_to_fit es True.\n",
    "        if self.resize_to_fit:\n",
    "            mask = edge_index[0] != -1\n",
    "            edge_index = edge_index[:, mask]\n",
    "            edge_weight = edge_weight[mask]\n",
    "            edge_vec = edge_vec[mask, :]\n",
    "\n",
    "        # Retorna los resultados, incluyendo los vectores de distancia si se habilita return_vecs.\n",
    "        if self.return_vecs:\n",
    "            return edge_index, edge_weight, edge_vec\n",
    "        else:\n",
    "            return edge_index, edge_weight, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase que se utiliza para aplicar un suavizado gaussiano a una distancia dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmearing(nn.Module):\n",
    "    # El método __init__ se llama al crear una instancia de la clase.\n",
    "    def __init__(self, cutoff_lower=0.0, cutoff_upper=5.0, num_rbf=50, trainable=True, dtype=torch.float32):\n",
    "        super(GaussianSmearing, self).__init__()\n",
    "        \n",
    "        # Guardar los valores de los argumentos en las variables de la instancia.\n",
    "        self.cutoff_lower = cutoff_lower\n",
    "        self.cutoff_upper = cutoff_upper\n",
    "        self.num_rbf = num_rbf\n",
    "        self.trainable = trainable\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Llamar al método _initial_params para obtener los valores iniciales de offset y coeff.\n",
    "        offset, coeff = self._initial_params()\n",
    "        \n",
    "        # Si trainable es True, se registran los parámetros como variables que se pueden entrenar.\n",
    "        if trainable:\n",
    "            self.register_parameter(\"coeff\", nn.Parameter(coeff))\n",
    "            self.register_parameter(\"offset\", nn.Parameter(offset))\n",
    "        # Si trainable es False, se registran los parámetros como buffers (variables no entrenables).\n",
    "        else:\n",
    "            self.register_buffer(\"coeff\", coeff)\n",
    "            self.register_buffer(\"offset\", offset)\n",
    "\n",
    "    # Método privado para calcular los valores iniciales de offset y coeff.\n",
    "    def _initial_params(self):\n",
    "        # Crear un tensor que contiene valores equidistantes entre cutoff_lower y cutoff_upper.\n",
    "        offset = torch.linspace(self.cutoff_lower, self.cutoff_upper, self.num_rbf, dtype=self.dtype)\n",
    "        \n",
    "        # Calcular el valor de coeff, que se utiliza en la función de suavizado gaussiano.\n",
    "        coeff = -0.5 / (offset[1] - offset[0]) ** 2\n",
    "        \n",
    "        return offset, coeff\n",
    "\n",
    "    # Método para restablecer los parámetros a sus valores iniciales.\n",
    "    def reset_parameters(self):\n",
    "        offset, coeff = self._initial_params()\n",
    "        self.offset.data.copy_(offset)\n",
    "        self.coeff.data.copy_(coeff)\n",
    "\n",
    "    # Método forward que se llama cuando se pasa una distancia (dist) a la instancia.\n",
    "    def forward(self, dist):\n",
    "        # Restar cada valor de dist de offset.\n",
    "        dist = dist.unsqueeze(-1) - self.offset\n",
    "        \n",
    "        # Calcular el suavizado gaussiano aplicando exp(coeff * dist^2).\n",
    "        return torch.exp(self.coeff * torch.pow(dist, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase que se utiliza para aplicar un suavizado exponencial a una distancia dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpNormalSmearing(nn.Module):\n",
    "    # El método __init__ se llama al crear una instancia de la clase.\n",
    "    def __init__(self, cutoff_lower=0.0, cutoff_upper=5.0, num_rbf=50, trainable=True, dtype=torch.float32):\n",
    "        super(ExpNormalSmearing, self).__init()\n",
    "        \n",
    "        # Guardar los valores de los argumentos en las variables de la instancia.\n",
    "        self.cutoff_lower = cutoff_lower\n",
    "        self.cutoff_upper = cutoff_upper\n",
    "        self.num_rbf = num_rbf\n",
    "        self.trainable = trainable\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Crear una instancia de la clase CosineCutoff con un rango de corte entre 0 y cutoff_upper.\n",
    "        self.cutoff_fn = CosineCutoff(0, cutoff_upper)\n",
    "        \n",
    "        # Calcular el valor de alpha para suavizar la función exponencial.\n",
    "        self.alpha = 5.0 / (cutoff_upper - cutoff_lower)\n",
    "        \n",
    "        # Llamar al método _initial_params para obtener los valores iniciales de means y betas.\n",
    "        means, betas = self._initial_params()\n",
    "        \n",
    "        # Si trainable es True, se registran los parámetros como variables que se pueden entrenar.\n",
    "        if trainable:\n",
    "            self.register_parameter(\"means\", nn.Parameter(means))\n",
    "            self.register_parameter(\"betas\", nn.Parameter(betas))\n",
    "        # Si trainable es False, se registran los parámetros como buffers (variables no entrenables).\n",
    "        else:\n",
    "            self.register_buffer(\"means\", means)\n",
    "            self.register_buffer(\"betas\", betas)\n",
    "\n",
    "    # Método privado para calcular los valores iniciales de means y betas.\n",
    "    def _initial_params(self):\n",
    "        # initialize means and betas according to the default values in PhysNet\n",
    "        # https://pubs.acs.org/doi/10.1021/acs.jctc.9b00181\n",
    "        start_value = torch.exp(\n",
    "            torch.scalar_tensor(-self.cutoff_upper + self.cutoff_lower, dtype=self.dtype)\n",
    "        )\n",
    "        means = torch.linspace(start_value, 1, self.num_rbf, dtype=self.dtype)\n",
    "        betas = torch.tensor(\n",
    "            [(2 / self.num_rbf * (1 - start_value)) ** -2] * self.num_rbf, dtype=self.dtype\n",
    "        )\n",
    "        return means, betas\n",
    "\n",
    "    # Método para restablecer los parámetros a sus valores iniciales.\n",
    "    def reset_parameters(self):\n",
    "        means, betas = self._initial_params()\n",
    "        self.means.data.copy_(means)\n",
    "        self.betas.data.copy_(betas)\n",
    "\n",
    "    # Método forward que se llama cuando se pasa una distancia (dist) a la instancia.\n",
    "    def forward(self, dist):\n",
    "        dist = dist.unsqueeze(-1)\n",
    "        \n",
    "        # Aplicar la función de corte (cutoff_fn) y el suavizado exponencial a la distancia.\n",
    "        return self.cutoff_fn(dist) * torch.exp(\n",
    "            -self.betas\n",
    "            * (torch.exp(self.alpha * (-dist + self.cutoff_lower)) - self.means) ** 2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una clase que es una subclase de nn.Module en PyTorch y se utiliza para aplicar la función Shifted Softplus a los valores de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedSoftplus(nn.Module):\n",
    "    r\"\"\"Applies the ShiftedSoftplus function :math:`\\text{ShiftedSoftplus}(x) = \\frac{1}{\\beta} *\n",
    "    \\log(1 + \\exp(\\beta * x))-\\log(2)` element-wise.\n",
    "\n",
    "    SoftPlus is a smooth approximation to the ReLU function and can be used\n",
    "    to constrain the output of a machine to always be positive.\n",
    "    \"\"\"\n",
    "    # El método __init__ se llama al crear una instancia de la clase.\n",
    "    def __init__(self):\n",
    "        super(ShiftedSoftplus, self).__init__()\n",
    "        # Calcular el valor de desplazamiento (shift) como el logaritmo de 2.\n",
    "        self.shift = torch.log(torch.tensor(2.0)).item()\n",
    "\n",
    "    # Método forward que se llama cuando se pasa un tensor x a la instancia.\n",
    "    def forward(self, x):\n",
    "        # Aplicar la función Softplus a los valores de entrada x y luego restar el desplazamiento (shift).\n",
    "        return F.softplus(x) - self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dfine una clase llamada CosineCutoff, que es una subclase de nn.Module en PyTorch y se utiliza para aplicar una función de corte basada en el coseno a las distancias de entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineCutoff(nn.Module):\n",
    "    # El método __init__ se llama al crear una instancia de la clase.\n",
    "    def __init__(self, cutoff_lower=0.0, cutoff_upper=5.0):\n",
    "        super(CosineCutoff, self).__init()\n",
    "        \n",
    "        # Guardar los valores de los argumentos en las variables de la instancia.\n",
    "        self.cutoff_lower = cutoff_lower\n",
    "        self.cutoff_upper = cutoff_upper\n",
    "\n",
    "    # Método forward que se llama cuando se pasa un tensor de distancias (distances) a la instancia.\n",
    "    def forward(self, distances):\n",
    "        # Comprueba si el valor de corte inferior (cutoff_lower) es mayor que cero.\n",
    "        if self.cutoff_lower > 0:\n",
    "            # Calcula los valores de corte basados en el coseno.\n",
    "            cutoffs = 0.5 * (\n",
    "                torch.cos(\n",
    "                    math.pi\n",
    "                    * (\n",
    "                        2\n",
    "                        * (distances - self.cutoff_lower)\n",
    "                        / (self.cutoff_upper - self.cutoff_lower)\n",
    "                        + 1.0\n",
    "                    )\n",
    "                )\n",
    "                + 1.0\n",
    "            )\n",
    "            # Elimina contribuciones por debajo del radio de corte.\n",
    "            cutoffs = cutoffs * (distances < self.cutoff_upper)\n",
    "            cutoffs = cutoffs * (distances > self.cutoff_lower)\n",
    "            return cutoffs\n",
    "        else:\n",
    "            # Calcula los valores de corte basados en el coseno sin corte inferior.\n",
    "            cutoffs = 0.5 * (torch.cos(distances * math.pi / self.cutoff_upper) + 1.0)\n",
    "            # Elimina contribuciones más allá del radio de corte.\n",
    "            cutoffs = cutoffs * (distances < self.cutoff_upper)\n",
    "            return cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dfine una clase llamada Distance, que es una subclase de nn.Module en PyTorch y se utiliza para calcular distancias entre átomos en una estructura atómica tridimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff_lower,\n",
    "        cutoff_upper,\n",
    "        max_num_neighbors=32,\n",
    "        return_vecs=False,\n",
    "        loop=False,\n",
    "    ):\n",
    "        super(Distance, self).__init__()\n",
    "\n",
    "        # Guardar los valores de los argumentos en las variables de la instancia.\n",
    "        self.cutoff_lower = cutoff_lower\n",
    "        self.cutoff_upper = cutoff_upper\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.return_vecs = return_vecs\n",
    "        self.loop = loop\n",
    "\n",
    "    # Método forward que se llama cuando se pasa un tensor de posiciones (pos) y una asignación de lotes (batch) a la instancia.\n",
    "    def forward(self, pos, batch):\n",
    "        # Utilizar la función `radius_graph` para encontrar los vecinos dentro del radio de corte.\n",
    "        edge_index = radius_graph(\n",
    "            pos,\n",
    "            r=self.cutoff_upper,\n",
    "            batch=batch,\n",
    "            loop=self.loop,\n",
    "            max_num_neighbors=self.max_num_neighbors + 1,\n",
    "        )\n",
    "\n",
    "        # Asegurarse de que no se hayan omitido vecinos debido a max_num_neighbors.\n",
    "        assert not (\n",
    "            torch.unique(edge_index[0], return_counts=True)[1] > self.max_num_neighbors\n",
    "        ).any(), (\n",
    "            \"The neighbor search missed some atoms due to max_num_neighbors being too low. \"\n",
    "            \"Please increase this parameter to include the maximum number of atoms within the cutoff.\"\n",
    "        )\n",
    "\n",
    "        # Calcular vectores de borde a partir de las posiciones de los átomos.\n",
    "        edge_vec = pos[edge_index[0]] - pos[edge_index[1]]\n",
    "\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "        if self.loop:\n",
    "            # Mascarar los bucles de autocorrección al calcular las distancias porque\n",
    "            # la norma de 0 produce gradientes NaN\n",
    "            # NOTA: podría influir en las predicciones de fuerza, ya que los gradientes de autocorrección se ignoran\n",
    "            mask = edge_index[0] != edge_index[1]\n",
    "            edge_weight = torch.zeros(\n",
    "                edge_vec.size(0), device=edge_vec.device, dtype=edge_vec.dtype\n",
    "            )\n",
    "            edge_weight[mask] = torch.norm(edge_vec[mask], dim=-1)\n",
    "        else:\n",
    "            edge_weight = torch.norm(edge_vec, dim=-1)\n",
    "        # Aplicar un umbral inferior para las distancias basado en el valor de cutoff_lower.\n",
    "        lower_mask = edge_weight >= self.cutoff_lower\n",
    "        if self.loop and mask is not None:\n",
    "            # Mantener los bucles de autocorrección incluso si están por debajo del umbral inferior.\n",
    "            lower_mask = lower_mask | ~mask\n",
    "        edge_index = edge_index[:, lower_mask]\n",
    "        edge_weight = edge_weight[lower_mask]\n",
    "\n",
    "        if self.return_vecs:\n",
    "            edge_vec = edge_vec[lower_mask]\n",
    "            return edge_index, edge_weight, edge_vec\n",
    "        # TODO: return only `edge_index` and `edge_weight` once\n",
    "        # Union typing works with TorchScript (https://github.com/pytorch/pytorch/pull/53180)\n",
    "        return edge_index, edge_weight, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedEquivariantBlock(nn.Module):\n",
    "    \"\"\"Gated Equivariant Block as defined in Schütt et al. (2021):\n",
    "    Equivariant message passing for the prediction of tensorial properties and molecular spectra\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        intermediate_channels=None,\n",
    "        activation=\"silu\",\n",
    "        scalar_activation=False,\n",
    "        dtype=torch.float,\n",
    "    ):\n",
    "        super(GatedEquivariantBlock, self).__init__()\n",
    "        # Guardar los canales de salida y otros hiperparámetros.\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if intermediate_channels is None:\n",
    "            intermediate_channels = hidden_channels\n",
    "\n",
    "        # Definir capas lineales para proyectar vectores de entrada.\n",
    "        self.vec1_proj = nn.Linear(hidden_channels, hidden_channels, bias=False, dtype=dtype)\n",
    "        self.vec2_proj = nn.Linear(hidden_channels, out_channels, bias=False, dtype=dtype)\n",
    "\n",
    "        act_class = act_class_mapping[activation]\n",
    "\n",
    "        # Definir una red secuencial para actualizar los vectores de entrada.\n",
    "        self.update_net = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, intermediate_channels, dtype=dtype),\n",
    "            act_class(),\n",
    "            nn.Linear(intermediate_channels, out_channels * 2, dtype=dtype),\n",
    "        )\n",
    "\n",
    "        # Definir una función de activación si se especifica.\n",
    "        self.act = act_class() if scalar_activation else None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Inicializar los parámetros de las capas lineales.\n",
    "        nn.init.xavier_uniform_(self.vec1_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.vec2_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.update_net[0].weight)\n",
    "        self.update_net[0].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.update_net[2].weight)\n",
    "        self.update_net[2].bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        # Proyectar el vector de entrada v.\n",
    "        vec1_buffer = self.vec1_proj(v)\n",
    "\n",
    "        # Separar vectores con ceros para evitar gradientes NaN durante la propagación hacia atrás.\n",
    "        vec1 = torch.zeros(\n",
    "            vec1_buffer.size(0), vec1_buffer.size(2), device=vec1_buffer.device, dtype=vec1_buffer.dtype\n",
    "        )\n",
    "        mask = (vec1_buffer != 0).view(vec1_buffer.size(0), -1).any(dim=1)\n",
    "        if not mask.all():\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    f\"Skipping gradients for {(~mask).sum()} atoms due to vector features being zero. \"\n",
    "                    \"This is likely due to atoms being outside the cutoff radius of any other atom. \"\n",
    "                    \"These atoms will not interact with any other atom unless you change the cutoff.\"\n",
    "                )\n",
    "            )\n",
    "        vec1[mask] = torch.norm(vec1_buffer[mask], dim=-2)\n",
    "\n",
    "        # Proyectar v nuevamente.\n",
    "        vec2 = self.vec2_proj(v)\n",
    "\n",
    "        # Concatenar x y vec1 y pasarlos por la red secuencial de actualización.\n",
    "        x = torch.cat([x, vec1], dim=-1)\n",
    "        x, v = torch.split(self.update_net(x), self.out_channels, dim=-1)\n",
    "\n",
    "        # Realizar la actualización de vectores con vec2 utilizando una compuerta.\n",
    "        v = v.unsqueeze(1) * vec2\n",
    "\n",
    "        if self.act is not None:\n",
    "            x = self.act(x)\n",
    "        return x, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de mapeo para funciones de base radial (RBF).\n",
    "rbf_class_mapping = {\n",
    "    \"gauss\": GaussianSmearing,    # Mapea \"gauss\" a la clase GaussianSmearing\n",
    "    \"expnorm\": ExpNormalSmearing   # Mapea \"expnorm\" a la clase ExpNormalSmearing\n",
    "}\n",
    "\n",
    "# Diccionario de mapeo para funciones de activación.\n",
    "act_class_mapping = {\n",
    "    \"ssp\": ShiftedSoftplus,    # Mapea \"ssp\" a la clase ShiftedSoftplus\n",
    "    \"silu\": nn.SiLU,           # Mapea \"silu\" a la clase nn.SiLU (Sigmoid-Weighted Linear Unit)\n",
    "    \"tanh\": nn.Tanh,           # Mapea \"tanh\" a la clase nn.Tanh (tangente hiperbólica)\n",
    "    \"sigmoid\": nn.Sigmoid     # Mapea \"sigmoid\" a la clase nn.Sigmoid (función sigmoide)\n",
    "}\n",
    "\n",
    "# Diccionario de mapeo para tipos de datos en PyTorch.\n",
    "dtype_mapping = {\n",
    "    16: torch.float16,   # Mapea 16 a torch.float16\n",
    "    32: torch.float,     # Mapea 32 a torch.float\n",
    "    64: torch.float64    # Mapea 64 a torch.float64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrenamiento: QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la biblioteca PyTorch, que se utiliza para crear y entrenar modelos de aprendizaje automático.\n",
    "import torch\n",
    "\n",
    "# Importamos algunas clases y funciones específicas de PyTorch Geometric que vamos a utilizar.\n",
    "# Compose se utiliza para componer transformaciones (transformers) en el conjunto de datos.\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "# Importamos el conjunto de datos QM9 de PyTorch Geometric.\n",
    "from torch_geometric.datasets import QM9 as QM9_geometric\n",
    "\n",
    "# Importamos el diccionario de objetivos (targets) relacionados con el conjunto de datos QM9\n",
    "# que se utiliza con el modelo Schnet en PyTorch Geometric.\n",
    "from torch_geometric.nn.models.schnet import qm9_target_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de una clase llamada QM9 que hereda de QM9_geometric, un conjunto de datos específico de PyTorch Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM9(QM9_geometric):\n",
    "    def __init__(self, root, transform=None, label=None):\n",
    "        # Creamos un diccionario que asigna nombres de propiedades químicas a índices.\n",
    "        label2idx = dict(zip(qm9_target_dict.values(), qm9_target_dict.keys()))\n",
    "        \n",
    "        # Verificamos que la propiedad química deseada esté en el diccionario.\n",
    "        assert label in label2idx, (\n",
    "            \"Please pass the desired property to \"\n",
    "            'train on via \"label\". Available '\n",
    "            f'properties are {\", \".join(label2idx)}.'\n",
    "        )\n",
    "        \n",
    "        # Guardamos la propiedad deseada en el objeto.\n",
    "        self.label = label\n",
    "        self.label_idx = label2idx[self.label]\n",
    "\n",
    "        # Si no se proporciona una transformación personalizada, usamos la transformación predeterminada (self._filter_label).\n",
    "        if transform is None:\n",
    "            transform = self._filter_label\n",
    "        else:\n",
    "            # Componemos la transformación personalizada junto con la predeterminada.\n",
    "            transform = Compose([transform, self._filter_label])\n",
    "\n",
    "        # Llamamos al constructor de la clase padre (QM9_geometric) para inicializar el conjunto de datos.\n",
    "        super(QM9, self).__init__(root, transform=transform)\n",
    "\n",
    "    def get_atomref(self, max_z=100):\n",
    "        # Llama a la función atomref para obtener los valores de referencia asociados a los índices de etiqueta (label_idx).\n",
    "        atomref = self.atomref(self.label_idx)\n",
    "\n",
    "        # Comprueba si no se encontraron valores de referencia y devuelve None si es así.\n",
    "        if atomref is None:\n",
    "            return None\n",
    "        # Comprueba si el tamaño de atomref no coincide con el valor máximo permitido (max_z).\n",
    "        if atomref.size(0) != max_z:\n",
    "            # Si no coinciden, crea un tensor de ceros de tamaño max_z y le asigna los valores de atomref hasta el índice mínimo entre max_z y el tamaño de atomref.\n",
    "            tmp = torch.zeros(max_z).unsqueeze(1)\n",
    "            idx = min(max_z, atomref.size(0))\n",
    "            tmp[:idx] = atomref[:idx]\n",
    "            return tmp\n",
    "        # Si atomref ya tiene el tamaño correcto, simplemente devuelve atomref.\n",
    "        return atomref\n",
    "\n",
    "    # Función para filtrar la etiqueta deseada en los datos.\n",
    "    def _filter_label(self, batch):\n",
    "        batch.y = batch.y[:, self.label_idx].unsqueeze(1)\n",
    "        return batch\n",
    "\n",
    "    # Función para descargar los datos del conjunto de datos (heredada de la clase padre).\n",
    "    def download(self):\n",
    "        super(QM9, self).download()\n",
    "\n",
    "    # Función para procesar los datos del conjunto de datos (heredada de la clase padre).\n",
    "    def process(self):\n",
    "        super(QM9, self).process()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os.path import join # Importamos la función 'join' desde el módulo 'os.path' para combinar rutas de archivos.\n",
    "from tqdm import tqdm # Importamos la función 'tqdm' para mostrar barras de progreso durante iteraciones.\n",
    "import torch # Importamos la biblioteca 'torch' para trabajar con PyTorch.\n",
    "from torch.utils.data import Subset # Importamos la clase 'Subset' desde 'torch.utils.data' para crear subconjuntos de datos.\n",
    "from torch_geometric.loader import DataLoader # Importamos la clase 'DataLoader' desde 'torch_geometric.loader' para cargar datos de manera eficiente.\n",
    "from pytorch_lightning import LightningDataModule # Importamos la clase 'LightningDataModule' desde 'pytorch_lightning' para manejar los datos en un proyecto PyTorch Lightning.\n",
    "from pytorch_lightning.utilities import rank_zero_warn # Importamos la función 'rank_zero_warn' desde 'pytorch_lightning.utilities' para mostrar advertencias en una única GPU.\n",
    "# from torchmdnet import datasets # Comentamos la importación de 'datasets' que parece estar comentada actualmente.\n",
    "from torch_geometric.data import Dataset # Importamos la clase 'Dataset' desde 'torch_geometric.data' para trabajar con conjuntos de datos en formato PyTorch Geometric.\n",
    "# from torchmdnet.utils import make_splits, MissingEnergyException # Comentamos la importación de 'make_splits' y 'MissingEnergyException' que parecen estar comentadas actualmente.\n",
    "from torch_scatter import scatter # Importamos la función 'scatter' desde 'torch_scatter' para realizar operaciones de dispersión en tensores.\n",
    "# from torchmdnet.models.utils import dtype_mapping # Comentamos la importación de 'dtype_mapping' que parece estar comentada actualmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatCastDatasetWrapper(Dataset):\n",
    "    def __init__(self, dataset, dtype=torch.float64):\n",
    "        # Llamamos al constructor de la clase base 'Dataset' y pasamos los atributos necesarios.\n",
    "        super(FloatCastDatasetWrapper, self).__init__(\n",
    "            dataset.root, dataset.transform, dataset.pre_transform, dataset.pre_filter\n",
    "        )\n",
    "        # Almacenamos una referencia al dataset original y el tipo de dato a utilizar.\n",
    "        self.dataset = dataset\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def len(self):\n",
    "        # Devuelve la longitud del dataset, que es la misma que la del dataset original.\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # Obtenemos un dato del dataset original en el índice 'idx'.\n",
    "        data = self.dataset.get(idx)\n",
    "        # Recorremos las claves y valores del dato.\n",
    "        for key, value in data:\n",
    "            # Verificamos si el valor es un tensor de punto flotante.\n",
    "            if torch.is_tensor(value) and torch.is_floating_point(value):\n",
    "                # Si es un tensor de punto flotante, lo convertimos al tipo de dato especificado.\n",
    "                setattr(data, key, value.to(self.dtype))\n",
    "        # Devolvemos el dato modificado.\n",
    "        return data\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # Comprobamos si el atributo existe en el dataset subyacente.\n",
    "        if hasattr(self.dataset, name):\n",
    "            # Si existe, lo obtenemos desde el dataset original.\n",
    "            return getattr(self.dataset, name)\n",
    "        # Si no existe, generamos un error de atributo.\n",
    "        raise AttributeError(\n",
    "            f\"'{type(self).__name__}' and its underlying dataset have no attribute '{name}'\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de una clase llamada DataModule que hereda de LightningDataModule\n",
    "class DataModule(LightningDataModule):\n",
    "    # Constructor de la clase, toma dos argumentos: hparams y dataset\n",
    "    def __init__(self, hparams, dataset=None):\n",
    "        # Llama al constructor de la clase padre (LightningDataModule)\n",
    "        super(DataModule, self).__init__()\n",
    "        \n",
    "        # Guarda los hiperparámetros (hparams) en la instancia de la clase\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        # Inicializa dos atributos (_mean y _std) como None\n",
    "        self._mean, self._std = None, None\n",
    "        \n",
    "        # Inicializa un diccionario llamado _saved_dataloaders para almacenar dataloaders\n",
    "        self._saved_dataloaders = dict()\n",
    "        \n",
    "        # Asigna el valor de dataset (si se proporciona) al atributo dataset de la clase\n",
    "        self.dataset = dataset\n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "     # Comienza la función 'setup' que se llama durante la configuración del módulo Lightning.\n",
    "        if self.dataset is None:\n",
    "            # Si 'self.dataset' aún no está configurado, significa que no se ha creado un conjunto de datos todavía.\n",
    "            if self.hparams[\"dataset\"] == \"Custom\":\n",
    "            # Si el nombre del conjunto de datos en los hiperparámetros es \"Custom\", se crea un conjunto de datos personalizado llamado 'Custom'.\n",
    "                self.dataset = datasets.Custom(\n",
    "                    self.hparams[\"coord_files\"],\n",
    "                    self.hparams[\"embed_files\"],\n",
    "                    self.hparams[\"energy_files\"],\n",
    "                    self.hparams[\"force_files\"],\n",
    "                )\n",
    "            else:\n",
    "                # Si el nombre del conjunto de datos no es \"Custom\", se utiliza el nombre del conjunto de datos de los hiperparámetros para crear un conjunto de datos predeterminado.\n",
    "                dataset_arg = {}\n",
    "                if self.hparams[\"dataset_arg\"] is not None:\n",
    "                    dataset_arg = self.hparams[\"dataset_arg\"]\n",
    "                self.dataset = QM9(\n",
    "                    self.hparams[\"dataset_root\"], **dataset_arg\n",
    "                )\n",
    "#                self.dataset = getattr(datasets, self.hparams[\"dataset\"])(\n",
    "#                    self.hparams[\"dataset_root\"], **dataset_arg\n",
    "#                )\n",
    "         # Después de crear el conjunto de datos, lo envolvemos en una clase llamada 'FloatCastDatasetWrapper' con el tipo de datos especificado en los hiperparámetros.\n",
    "        self.dataset = FloatCastDatasetWrapper(\n",
    "            self.dataset, dtype_mapping[self.hparams[\"precision\"]]\n",
    "        )\n",
    "        # Divide los índices del conjunto de datos en conjuntos de entrenamiento, validación y prueba utilizando la función make_splits.\n",
    "        self.idx_train, self.idx_val, self.idx_test = make_splits(\n",
    "            len(self.dataset),\n",
    "            self.hparams[\"train_size\"],\n",
    "            self.hparams[\"val_size\"],\n",
    "            self.hparams[\"test_size\"],\n",
    "            self.hparams[\"seed\"],\n",
    "            join(self.hparams[\"log_dir\"], \"splits.npz\"),\n",
    "            self.hparams[\"splits\"],\n",
    "        )\n",
    "\n",
    "        # Imprime las longitudes de los conjuntos de entrenamiento, validación y prueba.\n",
    "        print(\n",
    "            f\"train {len(self.idx_train)}, val {len(self.idx_val)}, test {len(self.idx_test)}\"\n",
    "        )\n",
    "\n",
    "        # Crea conjuntos de datos de entrenamiento, validación y prueba utilizando los índices generados.\n",
    "        self.train_dataset = Subset(self.dataset, self.idx_train)\n",
    "        self.val_dataset = Subset(self.dataset, self.idx_val)\n",
    "        self.test_dataset = Subset(self.dataset, self.idx_test)\n",
    "\n",
    "        # Si se establece 'standardize' en True en los hiperparámetros, se llama al método '_standardize'.\n",
    "        if self.hparams[\"standardize\"]:\n",
    "            self._standardize()\n",
    "\n",
    "    # Método para obtener el dataloader de entrenamiento\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")\n",
    "\n",
    "    # Método para obtener el dataloader de validación\n",
    "    def val_dataloader(self):\n",
    "        loaders = [self._get_dataloader(self.val_dataset, \"val\")]\n",
    "        # Comprueba si hay un conjunto de datos de prueba y si es necesario cargarlo, según la configuración de los hiperparámetros y la época actual del entrenamiento.\n",
    "        if (\n",
    "            len(self.test_dataset) > 0\n",
    "            and (self.trainer.current_epoch + 1) % self.hparams[\"test_interval\"] == 0\n",
    "        ):\n",
    "            loaders.append(self._get_dataloader(self.test_dataset, \"test\"))\n",
    "        return loaders\n",
    "\n",
    "    # Método para obtener el dataloader de prueba\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")\n",
    "\n",
    "    # Propiedad 'atomref' que proporciona una referencia a átomos (si está disponible en el conjunto de datos)\n",
    "    @property\n",
    "    def atomref(self):\n",
    "        if hasattr(self.dataset, \"get_atomref\"):\n",
    "            return self.dataset.get_atomref()\n",
    "        return None\n",
    "\n",
    "    # Propiedad 'mean' que proporciona el valor del atributo '_mean'\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._mean\n",
    "\n",
    "    # Propiedad 'std' que proporciona el valor del atributo '_std'\n",
    "    @property\n",
    "    def std(self):\n",
    "        return self._std\n",
    "\n",
    "    # Método privado '_get_dataloader' utilizado para obtener un dataloader para un conjunto de datos específico.\n",
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        # Verifica si se debe almacenar el dataloader en la memoria.\n",
    "        store_dataloader = (\n",
    "            store_dataloader and self.trainer.reload_dataloaders_every_n_epochs <= 0\n",
    "        )\n",
    "        # Comprueba si ya existe un dataloader almacenado para la etapa 'stage' y si se debe almacenar uno nuevo.\n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            # Si ya existe y se debe almacenar, se devuelve el dataloader existente para evitar recrearlo.\n",
    "            # storing the dataloaders like this breaks calls to trainer.reload_train_val_dataloaders\n",
    "            # but makes it possible that the dataloaders are not recreated on every testing epoch\n",
    "            return self._saved_dataloaders[stage]\n",
    "\n",
    "        # Configuración del tamaño de lote (batch_size) y si se debe realizar un barajado (shuffle) en función de la etapa (train, val, test).\n",
    "        if stage == \"train\":\n",
    "            batch_size = self.hparams[\"batch_size\"]\n",
    "            shuffle = True\n",
    "        elif stage in [\"val\", \"test\"]:\n",
    "            batch_size = self.hparams[\"inference_batch_size\"]\n",
    "            shuffle = False\n",
    "\n",
    "        # Creación de un DataLoader con las configuraciones especificadas.\n",
    "        dl = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        # Si se debe almacenar el dataloader, se guarda en el diccionario '_saved_dataloaders' para su uso futuro.\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        # Devuelve el dataloader recién creado o el almacenado si ya existía.\n",
    "        return dl\n",
    "\n",
    "    # Método privado '_standardize' para estandarizar los datos del conjunto de entrenamiento.\n",
    "    def _standardize(self):\n",
    "        # Función interna 'get_energy' para obtener las energías de un lote de datos y ajustarlas si existe una referencia a átomos.\n",
    "        def get_energy(batch, atomref):\n",
    "            if \"y\" not in batch or batch.y is None:\n",
    "                raise MissingEnergyException()\n",
    "\n",
    "            if atomref is None:\n",
    "                return batch.y.clone()\n",
    "\n",
    "            # Resta las energías de referencia de átomos de las energías objetivo.\n",
    "            # remove atomref energies from the target energy\n",
    "            atomref_energy = scatter(atomref[batch.z], batch.batch, dim=0)\n",
    "            return (batch.y.squeeze() - atomref_energy.squeeze()).clone()\n",
    "\n",
    "        # Utiliza un dataloader para calcular la media y la desviación estándar de las energías del conjunto de validación.\n",
    "        data = tqdm(\n",
    "            self._get_dataloader(self.train_dataset, \"val\", store_dataloader=False),\n",
    "            desc=\"computing mean and std\",\n",
    "        )\n",
    "        try:\n",
    "            # Solo elimina las energías de referencia de átomos si se utiliza el modelo de referencia a átomos (Atomref) en la configuración.\n",
    "            # only remove atomref energies if the atomref prior is used\n",
    "            atomref = self.atomref if self.hparams[\"prior_model\"] == \"Atomref\" else None\n",
    "            # Extrae las energías de los datos y las ajusta si existe una referencia a átomos.\n",
    "            # extract energies from the data\n",
    "            ys = torch.cat([get_energy(batch, atomref) for batch in data])\n",
    "        except MissingEnergyException:\n",
    "            # Advierte si la estandarización está habilitada pero no se pueden calcular la media y la desviación estándar, posiblemente porque el conjunto de datos solo contiene fuerzas en lugar de energías.\n",
    "            rank_zero_warn(\n",
    "                \"Standardize is true but failed to compute dataset mean and \"\n",
    "                \"standard deviation. Maybe the dataset only contains forces.\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # Calcula la media y la desviación estándar de las energías.\n",
    "        # compute mean and standard deviation\n",
    "        self._mean = ys.mean(dim=0)\n",
    "        self._std = ys.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envoltorios o encapsulados (Wrappers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de módulos y clases necesarios desde las bibliotecas estándar de Python y PyTorch.\n",
    "from abc import abstractmethod, ABCMeta  # Importa las clases para definir clases abstractas y metaclasses.\n",
    "from typing import Optional, Tuple  # Importa tipos opcionales y tuplas.\n",
    "from torch import nn, Tensor  # Importa el módulo de redes neuronales y el tipo Tensor de PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código define una clase abstracta llamada BaseWrapper, que sirve como una clase base para envolver modelos en el contexto de redes neuronales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase abstracta BaseWrapper que hereda de nn.Module y utiliza la metacalse ABCMeta.\n",
    "class BaseWrapper(nn.Module, metaclass=ABCMeta):\n",
    "    r\"\"\"Base class for model wrappers.\n",
    "\n",
    "    Children of this class should implement the `forward` method,\n",
    "    which calls `self.model(z, pos, batch=batch)` at some point.\n",
    "    Wrappers that are applied before the REDUCE operation should return\n",
    "    the model's output, `z`, `pos`, `batch` and potentially vector\n",
    "    features`v`. Wrappers that are applied after REDUCE should only\n",
    "    return the model's output.\n",
    "    \"\"\"\n",
    "    # Constructor de la clase BaseWrapper que toma un modelo como argumento.\n",
    "    def __init__(self, model):\n",
    "        super(BaseWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    # Método para restablecer los parámetros del modelo.\n",
    "    def reset_parameters(self):\n",
    "        self.model.reset_parameters()\n",
    "\n",
    "    # Método abstracto que debe ser implementado por las clases hijas.\n",
    "    @abstractmethod\n",
    "    def forward(self, z, pos, batch=None):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código define una clase llamada AtomFilter, que hereda de BaseWrapper. La clase AtomFilter se utiliza para aplicar un filtro a la salida del modelo, eliminando átomos con una carga atómica (Z) mayor que un umbral específico (remove_threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase AtomFilter que hereda de BaseWrapper.\n",
    "class AtomFilter(BaseWrapper):\n",
    "    \"\"\"\n",
    "    Remove atoms with Z > remove_threshold from the model's output.\n",
    "    \"\"\"\n",
    "    # Constructor de la clase AtomFilter que toma un modelo y un umbral de eliminación como argumentos.\n",
    "    def __init__(self, model, remove_threshold):\n",
    "        super(AtomFilter, self).__init__(model)\n",
    "        self.remove_threshold = remove_threshold\n",
    "\n",
    "    # Método 'forward' para aplicar el filtro de átomos.\n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        pos: Tensor,\n",
    "        batch: Tensor,\n",
    "        q: Optional[Tensor] = None,\n",
    "        s: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor], Tensor, Tensor, Tensor]:\n",
    "        # Llama al modelo subyacente y obtiene la salida.\n",
    "        x, v, z, pos, batch = self.model(z, pos, batch=batch, q=q, s=s)\n",
    "\n",
    "        n_samples = len(batch.unique())\n",
    "\n",
    "        # Aplica el filtro de átomos eliminando aquellos con Z > remove_threshold.\n",
    "        # drop atoms according to the filter\n",
    "        atom_mask = z > self.remove_threshold\n",
    "        x = x[atom_mask]\n",
    "        if v is not None:\n",
    "            v = v[atom_mask]\n",
    "        z = z[atom_mask]\n",
    "        pos = pos[atom_mask]\n",
    "        batch = batch[atom_mask]\n",
    "\n",
    "        # Asegura que no se eliminen por completo muestras, al menos un átomo con Z > remove_threshold por muestra.\n",
    "        assert len(batch.unique()) == n_samples, (\n",
    "            \"Some samples were completely filtered out by the atom filter. \"\n",
    "            f\"Make sure that at least one atom per sample exists with Z > {self.remove_threshold}.\"\n",
    "        )\n",
    "        return x, v, z, pos, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de módulos y clases necesarios desde las bibliotecas estándar de Python y PyTorch.\n",
    "from abc import abstractmethod, ABCMeta  # Importa las clases para definir clases abstractas y metaclasses.\n",
    "from torch_scatter import scatter  # Importa la función 'scatter' desde 'torch_scatter'.\n",
    "from typing import Optional  # Importa el tipo de datos opcional para indicar que un valor puede ser None.\n",
    "#import torchmdnet.models.utils.act_class_mapping, GatedEquivariantBlock  # Importaciones que están actualmente comentadas.\n",
    "#import torchmdnet.utils.atomic_masses  # Importación que está actualmente comentada.\n",
    "import torch  # Importa el módulo 'torch' de PyTorch.\n",
    "from torch import nn  # Importa el módulo de redes neuronales de PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código define la clase OutputModel, que es una clase abstracta destinada a representar modelos de salida en el contexto de un proyecto de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase OutputModel que hereda de nn.Module y utiliza la metacalse ABCMeta.\n",
    "class OutputModel(nn.Module, metaclass=ABCMeta):\n",
    "    # Constructor de la clase OutputModel que toma como argumentos 'allow_prior_model' y 'reduce_op'.\n",
    "    def __init__(self, allow_prior_model, reduce_op):\n",
    "        super(OutputModel, self).__init__()\n",
    "        self.allow_prior_model = allow_prior_model\n",
    "        self.reduce_op = reduce_op\n",
    "\n",
    "    # Método para restablecer los parámetros del modelo (método vacío).\n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "\n",
    "    # Método abstracto 'pre_reduce' que debe ser implementado por las clases hijas.\n",
    "    @abstractmethod\n",
    "    def pre_reduce(self, x, v, z, pos, batch):\n",
    "        return\n",
    "\n",
    "    # Método 'reduce' que utiliza la función 'scatter' para realizar una operación de reducción.\n",
    "    def reduce(self, x, batch):\n",
    "        return scatter(x, batch, dim=0, reduce=self.reduce_op)\n",
    "\n",
    "    # Método 'post_reduce' que devuelve la salida sin cambios.\n",
    "    def post_reduce(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este fragmento de código se define la clase Scalar, que es un tipo de modelo de salida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible\n",
    "# Definición de la clase Scalar que hereda de OutputModel.\n",
    "class Scalar(OutputModel):\n",
    "    # Constructor de la clase Scalar que toma varios argumentos para configurar el modelo.\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        activation=\"silu\",\n",
    "        allow_prior_model=True,\n",
    "        reduce_op=\"sum\",\n",
    "        dtype=torch.float\n",
    "    ):\n",
    "        super(Scalar, self).__init__(\n",
    "            allow_prior_model=allow_prior_model, reduce_op=reduce_op\n",
    "        )\n",
    "        \n",
    "        # Mapeo de la función de activación a una clase correspondiente.\n",
    "        act_class = act_class_mapping[activation]\n",
    "\n",
    "        # Definición de la red de salida como una secuencia de capas lineales y funciones de activación.\n",
    "        self.output_network = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2, dtype=dtype),\n",
    "            act_class(),\n",
    "            nn.Linear(hidden_channels // 2, 1, dtype=dtype),\n",
    "        )\n",
    "\n",
    "        # Llama al método reset_parameters para inicializar los parámetros de la red de salida.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Método para restablecer los parámetros de la red de salida.\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.output_network[0].weight)\n",
    "        self.output_network[0].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.output_network[2].weight)\n",
    "        self.output_network[2].bias.data.fill_(0)\n",
    "\n",
    "    # Método 'pre_reduce' para realizar operaciones antes de la reducción en los datos.\n",
    "    def pre_reduce(self, x, v: Optional[torch.Tensor], z, pos, batch):\n",
    "        return self.output_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase EquivariantScalar que hereda de OutputModel.\n",
    "class EquivariantScalar(OutputModel):\n",
    "    # Constructor de la clase EquivariantScalar que toma varios argumentos para configurar el modelo.\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        activation=\"silu\",\n",
    "        allow_prior_model=True,\n",
    "        reduce_op=\"sum\",\n",
    "        dtype=torch.float\n",
    "    ):\n",
    "        super(EquivariantScalar, self).__init__(\n",
    "            allow_prior_model=allow_prior_model, reduce_op=reduce_op\n",
    "        )\n",
    "        \n",
    "        # Definición de la red de salida como una secuencia de bloques equivariantes y activaciones.\n",
    "        self.output_network = nn.ModuleList(\n",
    "            [\n",
    "                GatedEquivariantBlock(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels // 2,\n",
    "                    activation=activation,\n",
    "                    scalar_activation=True,\n",
    "                    dtype=dtype\n",
    "                ),\n",
    "                GatedEquivariantBlock(hidden_channels // 2, 1, activation=activation, dtype=dtype),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Llama al método reset_parameters para inicializar los parámetros de la red de salida.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Método para restablecer los parámetros de la red de salida.\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.output_network:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    # Método 'pre_reduce' para realizar operaciones antes de la reducción en los datos.\n",
    "    def pre_reduce(self, x, v, z, pos, batch):\n",
    "        for layer in self.output_network:\n",
    "            x, v = layer(x, v)\n",
    "        # Incluye 'v' en la salida para asegurar que todos los parámetros tengan un gradiente.\n",
    "        # include v in output to make sure all parameters have a gradient\n",
    "        return x + v.sum() * 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código define la clase DipoleMoment, que hereda de la clase Scalar. La clase DipoleMoment se utiliza para calcular el momento dipolar en un sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible\n",
    "# Definición de la clase DipoleMoment que hereda de Scalar.\n",
    "class DipoleMoment(Scalar):\n",
    "    # Constructor de la clase DipoleMoment que toma varios argumentos para configurar el modelo.\n",
    "    def __init__(self, hidden_channels, activation=\"silu\", reduce_op=\"sum\", dtype=torch.float):\n",
    "        super(DipoleMoment, self).__init__(\n",
    "            hidden_channels, activation, allow_prior_model=False, reduce_op=reduce_op, dtype=dtype\n",
    "        )\n",
    "        \n",
    "        # Cálculo de la masa atómica a partir de 'atomic_masses' y registro como un búfer en el modelo.\n",
    "        atomic_mass = torch.from_numpy(atomic_masses).to(dtype)\n",
    "        self.register_buffer(\"atomic_mass\", atomic_mass)\n",
    "\n",
    "    # Método 'pre_reduce' para realizar operaciones antes de la reducción en los datos.\n",
    "    def pre_reduce(self, x, v: Optional[torch.Tensor], z, pos, batch):\n",
    "        x = self.output_network(x)\n",
    "\n",
    "        # Cálculo del centro de masas.\n",
    "        # Get center of mass.\n",
    "        mass = self.atomic_mass[z].view(-1, 1)\n",
    "        c = scatter(mass * pos, batch, dim=0) / scatter(mass, batch, dim=0)\n",
    "        \n",
    "        # Cálculo del momento dipolar.\n",
    "        x = x * (pos - c[batch])\n",
    "        return x\n",
    "\n",
    "    # Método 'post_reduce' para realizar operaciones después de la reducción en los datos.\n",
    "    def post_reduce(self, x):\n",
    "        # Cálculo de la norma del momento dipolar.\n",
    "        return torch.norm(x, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EquivariantDipoleMoment es una clase especializada para calcular el momento dipolar en sistemas que requieren consideraciones de invariancia y transformaciones específicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase EquivariantDipoleMoment que hereda de EquivariantScalar.\n",
    "class EquivariantDipoleMoment(EquivariantScalar):\n",
    "    # Constructor de la clase EquivariantDipoleMoment que toma varios argumentos para configurar el modelo.\n",
    "    def __init__(self, hidden_channels, activation=\"silu\", reduce_op=\"sum\", dtype=torch.float):\n",
    "        super(EquivariantDipoleMoment, self).__init__(\n",
    "            hidden_channels, activation, allow_prior_model=False, reduce_op=reduce_op, dtype=dtype\n",
    "        )\n",
    "        \n",
    "        # Cálculo de la masa atómica a partir de 'atomic_masses' y registro como un búfer en el modelo.\n",
    "        atomic_mass = torch.from_numpy(atomic_masses).to(dtype)\n",
    "        self.register_buffer(\"atomic_mass\", atomic_mass)\n",
    "\n",
    "    # Método 'pre_reduce' para realizar operaciones antes de la reducción en los datos.\n",
    "    def pre_reduce(self, x, v, z, pos, batch):\n",
    "        for layer in self.output_network:\n",
    "            x, v = layer(x, v)\n",
    "\n",
    "        # Cálculo del centro de masas.\n",
    "        # Get center of mass.\n",
    "        mass = self.atomic_mass[z].view(-1, 1)\n",
    "        c = scatter(mass * pos, batch, dim=0) / scatter(mass, batch, dim=0)\n",
    "        # Cálculo del momento dipolar.\n",
    "        x = x * (pos - c[batch])\n",
    "        \n",
    "        # Añade 'v' a la salida para asegurar que todos los parámetros tengan un gradiente.\n",
    "        return x + v.squeeze()\n",
    "\n",
    "    # Método 'post_reduce' para realizar operaciones después de la reducción en los datos.\n",
    "    def post_reduce(self, x):\n",
    "        # Cálculo de la norma del momento dipolar.\n",
    "        return torch.norm(x, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElectronicSpatialExtent es una clase que calcula y representa la extensión espacial electrónica de un sistema, teniendo en cuenta las posiciones de los átomos y la masa atómica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible\n",
    "# Definición de la clase ElectronicSpatialExtent que hereda de OutputModel.\n",
    "class ElectronicSpatialExtent(OutputModel):\n",
    "    # Constructor de la clase ElectronicSpatialExtent que toma varios argumentos para configurar el modelo.\n",
    "    def __init__(self, hidden_channels, activation=\"silu\", reduce_op=\"sum\", dtype=torch.float):\n",
    "        super(ElectronicSpatialExtent, self).__init__(\n",
    "            allow_prior_model=False, reduce_op=reduce_op\n",
    "        )\n",
    "        \n",
    "        # Mapeo de la función de activación a una clase correspondiente.\n",
    "        act_class = act_class_mapping[activation]\n",
    "\n",
    "        # Definición de la red de salida como una secuencia de capas lineales y funciones de activación.\n",
    "        self.output_network = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2, dtype=dtype),\n",
    "            act_class(),\n",
    "            nn.Linear(hidden_channels // 2, 1, dtype=dtype),\n",
    "        )\n",
    "\n",
    "        # Cálculo de la masa atómica a partir de 'atomic_masses' y registro como un búfer en el modelo.\n",
    "        atomic_mass = torch.from_numpy(atomic_masses).to(dtype)\n",
    "        self.register_buffer(\"atomic_mass\", atomic_mass)\n",
    "\n",
    "        # Llama al método reset_parameters para inicializar los parámetros de la red de salida.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Método para restablecer los parámetros de la red de salida.\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.output_network[0].weight)\n",
    "        self.output_network[0].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.output_network[2].weight)\n",
    "        self.output_network[2].bias.data.fill_(0)\n",
    "\n",
    "    # Método 'pre_reduce' para realizar operaciones antes de la reducción en los datos.\n",
    "    def pre_reduce(self, x, v: Optional[torch.Tensor], z, pos, batch):\n",
    "        x = self.output_network(x)\n",
    "\n",
    "        # Cálculo del centro de masas.\n",
    "        # Get center of mass.\n",
    "        mass = self.atomic_mass[z].view(-1, 1)\n",
    "        c = scatter(mass * pos, batch, dim=0) / scatter(mass, batch, dim=0)\n",
    "\n",
    "        # Cálculo de la distancia al cuadrado entre las posiciones y el centro de masas.\n",
    "        x = torch.norm(pos - c[batch], dim=1, keepdim=True) ** 2 * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantElectronicSpatialExtent(ElectronicSpatialExtent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EquivariantVectorOutput es una subclase de EquivariantScalar. Aunque la clase base se llama \"EquivariantScalar\", esta subclase está diseñada para manejar salidas vectoriales en lugar de escalares. Esto significa que se utiliza para generar vectores de salida en lugar de un solo valor escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase EquivariantVectorOutput que hereda de EquivariantScalar.\n",
    "class EquivariantVectorOutput(EquivariantScalar):\n",
    "    # Constructor de la clase EquivariantVectorOutput que toma varios argumentos para configurar el modelo.\n",
    "    def __init__(self, hidden_channels, activation=\"silu\", reduce_op=\"sum\", dtype=torch.float):\n",
    "        super(EquivariantVectorOutput, self).__init__(\n",
    "            hidden_channels, activation, allow_prior_model=False, reduce_op=\"sum\", dtype=dtype\n",
    "        )\n",
    "\n",
    "    # Método 'pre_reduce' para realizar operaciones antes de la reducción en los datos.\n",
    "    def pre_reduce(self, x, v, z, pos, batch):\n",
    "        for layer in self.output_network:\n",
    "            x, v = layer(x, v)\n",
    "        \n",
    "        # Devuelve 'v' después de aplicar las capas de la red de salida.\n",
    "        return v.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base previos (Priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las clases 'nn' y 'Tensor' desde el módulo 'torch'\n",
    "# Estas clases son proporcionadas por PyTorch, un popular marco de trabajo para el aprendizaje profundo.\n",
    "from torch import nn, Tensor\n",
    "\n",
    "# Importamos el tipo 'Optional' y el tipo 'Dict' desde el módulo 'typing'\n",
    "# Estos tipos se utilizan para proporcionar información adicional sobre las variables y argumentos en el código.\n",
    "from typing import Optional, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " define una clase llamada BasePrior, que sirve como una plantilla base para modelos de prior. Los modelos de prior son utilizados en aplicaciones de aprendizaje profundo para preprocesar predicciones atomísticas o moleculares antes de ser utilizadas en tareas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una clase llamada 'BasePrior' que hereda de 'nn.Module', lo que significa que es un módulo de PyTorch.\n",
    "class BasePrior(nn.Module):\n",
    "    r\"\"\"Base class for prior models.\n",
    "    Derive this class to make custom prior models, which take some arguments and a dataset as input.\n",
    "    As an example, have a look at the `torchmdnet.priors.atomref.Atomref` prior.\n",
    "    \"\"\"\n",
    "    # El método '__init__' es el constructor de la clase.\n",
    "    def __init__(self, dataset=None):\n",
    "        # Llamamos al constructor de la clase base 'nn.Module' utilizando 'super()'.\n",
    "        super().__init__()\n",
    "\n",
    "    # Definimos un método llamado 'get_init_args'.\n",
    "    def get_init_args(self):\n",
    "        r\"\"\"A function that returns all required arguments to construct a prior object.\n",
    "        The values should be returned inside a dict with the keys being the arguments' names.\n",
    "        All values should also be saveable in a .yaml file as this is used to reconstruct the\n",
    "        prior model from a checkpoint file.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "    \n",
    "    # Definimos un método llamado 'pre_reduce'.\n",
    "    def pre_reduce(self, x, z, pos, batch, extra_args: Optional[Dict[str, Tensor]]):\n",
    "        r\"\"\"Pre-reduce method of the prior model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): scalar atom-wise predictions from the model.\n",
    "            z (torch.Tensor): atom types of all atoms.\n",
    "            pos (torch.Tensor): 3D atomic coordinates.\n",
    "            batch (torch.Tensor): tensor containing the sample index for each atom.\n",
    "            extra_args (dict): any addition fields provided by the dataset\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: updated scalar atom-wise predictions\n",
    "        \"\"\"\n",
    "        return x\n",
    "    \n",
    "        # Este método recibe varios argumentos y devuelve un tensor 'x'. \n",
    "        # Puede ser usado para realizar operaciones de preprocesamiento en las predicciones atomísticas.\n",
    "\n",
    "    # Definimos un método llamado 'post_reduce'.\n",
    "    def post_reduce(self, y, z, pos, batch, extra_args: Optional[Dict[str, Tensor]]):\n",
    "        r\"\"\"Post-reduce method of the prior model.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): scalar molecule-wise predictions from the model.\n",
    "            z (torch.Tensor): atom types of all atoms.\n",
    "            pos (torch.Tensor): 3D atomic coordinates.\n",
    "            batch (torch.Tensor): tensor containing the sample index for each atom.\n",
    "            extra_args (dict): any addition fields provided by the dataset\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: updated scalar molecular-wise predictions\n",
    "        \"\"\"\n",
    "        return y\n",
    "    \n",
    "# La clase 'BasePrior' actúa como una plantilla base para la creación de modelos de prior específicos que heredan de ella.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors: Atomref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmdnet.priors.base import BasePrior\n",
    "from typing import Optional, Dict  # Importamos tipos para anotaciones de funciones.\n",
    "import torch  # Importamos PyTorch, un marco de trabajo para aprendizaje profundo.\n",
    "from torch import nn, Tensor  # Importamos clases y tipos específicos de PyTorch.\n",
    "from pytorch_lightning.utilities import rank_zero_warn  # Importamos una función para emitir advertencias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta parte del código define una clase llamada 'Atomref' que hereda de 'BasePrior'. La clase 'Atomref' se utiliza para implementar un modelo de referencia de átomos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atomref(BasePrior):\n",
    "    r\"\"\"Atomref prior model.\n",
    "    When using this in combination with some dataset, the dataset class must implement\n",
    "    the function `get_atomref`, which returns the atomic reference values as a tensor.\n",
    "    \"\"\"\n",
    "    # El método '__init__' es el constructor de la clase.\n",
    "    def __init__(self, max_z=None, dataset=None):\n",
    "        # Llamamos al constructor de la clase base 'BasePrior' utilizando 'super()'.\n",
    "        super().__init__()\n",
    "        # Comprobamos los argumentos pasados al constructor.\n",
    "        if max_z is None and dataset is None:\n",
    "            # Si tanto 'max_z' como 'dataset' son 'None', lanzamos una excepción ValueError.\n",
    "            raise ValueError(\"Can't instantiate Atomref prior, all arguments are None.\")\n",
    "        if dataset is None:\n",
    "            # Si 'dataset' es 'None', creamos un tensor de referencia de átomos lleno de ceros.\n",
    "            atomref = torch.zeros(max_z, 1)\n",
    "        else:\n",
    "            # Si 'dataset' no es 'None', obtenemos el tensor de referencia de átomos utilizando el método 'get_atomref' del dataset.\n",
    "            atomref = dataset.get_atomref()\n",
    "            if atomref is None:\n",
    "                # Si el tensor de referencia de átomos es 'None', emitimos una advertencia y utilizamos un tensor lleno de ceros con un máximo número atómico de 99.\n",
    "                rank_zero_warn(\n",
    "                    \"The atomref returned by the dataset is None, defaulting to zeros with max. \"\n",
    "                    \"atomic number 99. Maybe atomref is not defined for the current target.\"\n",
    "                )\n",
    "                atomref = torch.zeros(100, 1)\n",
    "\n",
    "        if atomref.ndim == 1:\n",
    "            # Si el tensor de referencia de átomos tiene una dimensión, lo redimensionamos a una columna.\n",
    "            atomref = atomref.view(-1, 1)\n",
    "\n",
    "        # Registramos el tensor de referencia de átomos como un buffer para que se incluya en la lista de parámetros del modelo.\n",
    "        self.register_buffer(\"initial_atomref\", atomref)\n",
    "        # Creamos una capa de embedding llamada 'atomref' que asigna índices a valores de referencia de átomos.\n",
    "        self.atomref = nn.Embedding(len(atomref), 1)\n",
    "        # Inicializamos los pesos de la capa de embedding con el tensor de referencia de átomos.\n",
    "        self.atomref.weight.data.copy_(atomref)\n",
    "\n",
    "    # Definimos un método llamado 'reset_parameters'.\n",
    "    def reset_parameters(self):\n",
    "        # Restablecemos los pesos de la capa de embedding 'atomref' a los valores iniciales.\n",
    "        self.atomref.weight.data.copy_(self.initial_atomref)\n",
    "\n",
    "    # Definimos un método llamado 'get_init_args'.\n",
    "    def get_init_args(self):\n",
    "        # Este método devuelve un diccionario con el valor de 'max_z', que es la longitud del tensor de referencia de átomos.\n",
    "        return dict(max_z=self.initial_atomref.size(0))\n",
    "\n",
    "    # Definimos un método llamado 'pre_reduce'.\n",
    "    def pre_reduce(self, x: Tensor, z: Tensor, pos: Tensor, batch: Tensor, extra_args: Optional[Dict[str, Tensor]]):\n",
    "        # Este método toma predicciones 'x', tipos de átomos 'z', coordenadas 'pos', índices de lotes 'batch', y argumentos adicionales 'extra_args'.\n",
    "        # Luego, ajusta las predicciones 'x' agregando los valores de referencia de átomos correspondientes a los tipos de átomos 'z'.\n",
    "        return x + self.atomref(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: equivariant-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los siguientes módulos y tipos para su uso en el código.\n",
    "from typing import Optional, Tuple  # Importamos tipos para anotaciones de funciones.\n",
    "import torch  # Importamos PyTorch, un marco de trabajo para aprendizaje profundo.\n",
    "from torch import Tensor, nn  # Importamos clases y tipos específicos de PyTorch.\n",
    "from torch_geometric.nn import MessagePassing  # Importamos una clase de PyTorch Geometric para pasar mensajes entre nodos en grafos.\n",
    "from torch_scatter import scatter  # Importamos una función para realizar reducciones en datos dispersos.\n",
    "#from torchmdnet.models.utils import (\n",
    "#    NeighborEmbedding,\n",
    "#    CosineCutoff,\n",
    "#    Distance,\n",
    "#    rbf_class_mapping,\n",
    "#    act_class_mapping,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchMD_ET(nn.Module):\n",
    "    r\"\"\"The TorchMD equivariant Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        hidden_channels (int, optional): Hidden embedding size.\n",
    "            (default: :obj:`128`)\n",
    "        num_layers (int, optional): The number of attention layers.\n",
    "            (default: :obj:`6`)\n",
    "        num_rbf (int, optional): The number of radial basis functions :math:`\\mu`.\n",
    "            (default: :obj:`50`)\n",
    "        rbf_type (string, optional): The type of radial basis function to use.\n",
    "            (default: :obj:`\"expnorm\"`)\n",
    "        trainable_rbf (bool, optional): Whether to train RBF parameters with\n",
    "            backpropagation. (default: :obj:`True`)\n",
    "        activation (string, optional): The type of activation function to use.\n",
    "            (default: :obj:`\"silu\"`)\n",
    "        attn_activation (string, optional): The type of activation function to use\n",
    "            inside the attention mechanism. (default: :obj:`\"silu\"`)\n",
    "        neighbor_embedding (bool, optional): Whether to perform an initial neighbor\n",
    "            embedding step. (default: :obj:`True`)\n",
    "        num_heads (int, optional): Number of attention heads.\n",
    "            (default: :obj:`8`)\n",
    "        distance_influence (string, optional): Where distance information is used inside\n",
    "            the attention mechanism. (default: :obj:`\"both\"`)\n",
    "        cutoff_lower (float, optional): Lower cutoff distance for interatomic interactions.\n",
    "            (default: :obj:`0.0`)\n",
    "        cutoff_upper (float, optional): Upper cutoff distance for interatomic interactions.\n",
    "            (default: :obj:`5.0`)\n",
    "        max_z (int, optional): Maximum atomic number. Used for initializing embeddings.\n",
    "            (default: :obj:`100`)\n",
    "        max_num_neighbors (int, optional): Maximum number of neighbors to return for a\n",
    "            given node/atom when constructing the molecular graph during forward passes.\n",
    "            This attribute is passed to the torch_cluster radius_graph routine keyword\n",
    "            max_num_neighbors, which normally defaults to 32. Users should set this to\n",
    "            higher values if they are using higher upper distance cutoffs and expect more\n",
    "            than 32 neighbors per node/atom.\n",
    "            (default: :obj:`32`)\n",
    "    \"\"\"\n",
    "\n",
    "    # El método '__init__' es el constructor de la clase y se utiliza para inicializar sus atributos.\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels=128,\n",
    "        num_layers=6,\n",
    "        num_rbf=50,\n",
    "        rbf_type=\"expnorm\",\n",
    "        trainable_rbf=True,\n",
    "        activation=\"silu\",\n",
    "        attn_activation=\"silu\",\n",
    "        neighbor_embedding=True,\n",
    "        num_heads=8,\n",
    "        distance_influence=\"both\",\n",
    "        cutoff_lower=0.0,\n",
    "        cutoff_upper=5.0,\n",
    "        max_z=100,\n",
    "        max_num_neighbors=32,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        # Llamamos al constructor de la clase base 'nn.Module'.\n",
    "        super(TorchMD_ET, self).__init()\n",
    "\n",
    "        # Validamos los valores de algunos de los parámetros pasados al constructor.\n",
    "        assert distance_influence in [\"keys\", \"values\", \"both\", \"none\"]\n",
    "        assert rbf_type in rbf_class_mapping, (\n",
    "            f'Unknown RBF type \"{rbf_type}\". '\n",
    "            f'Choose from {\", \".join(rbf_class_mapping.keys())}.'\n",
    "        )\n",
    "        assert activation in act_class_mapping, (\n",
    "            f'Unknown activation function \"{activation}\". '\n",
    "            f'Choose from {\", \".join(act_class_mapping.keys())}.'\n",
    "        )\n",
    "        assert attn_activation in act_class_mapping, (\n",
    "            f'Unknown attention activation function \"{attn_activation}\". '\n",
    "            f'Choose from {\", \".join(act_class_mapping.keys())}.'\n",
    "        )\n",
    "\n",
    "        # Asignamos los parámetros pasados al constructor como atributos de la clase.\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.num_rbf = num_rbf\n",
    "        self.rbf_type = rbf_type\n",
    "        self.trainable_rbf = trainable_rbf\n",
    "        self.activation = activation\n",
    "        self.attn_activation = attn_activation\n",
    "        self.neighbor_embedding = neighbor_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.distance_influence = distance_influence\n",
    "        self.cutoff_lower = cutoff_lower\n",
    "        self.cutoff_upper = cutoff_upper\n",
    "        self.max_z = max_z\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Obtenemos la clase de función de activación adecuada según el parámetro 'activation'.\n",
    "        act_class = act_class_mapping[activation]\n",
    "\n",
    "        # Creamos una capa de embedding llamada 'embedding' para representar los tipos de átomos.\n",
    "        self.embedding = nn.Embedding(self.max_z, hidden_channels, dtype=dtype)\n",
    "\n",
    "        # Creamos una instancia de la clase 'Distance' para calcular distancias entre átomos.\n",
    "        self.distance = Distance(\n",
    "            cutoff_lower,\n",
    "            cutoff_upper,\n",
    "            max_num_neighbors=max_num_neighbors,\n",
    "            return_vecs=True,\n",
    "            loop=True,\n",
    "        )\n",
    "\n",
    "        # Creamos una instancia de la función de base radial (RBF) para la expansión de distancia.\n",
    "        self.distance_expansion = rbf_class_mapping[rbf_type](\n",
    "            cutoff_lower, cutoff_upper, num_rbf, trainable_rbf\n",
    "        )\n",
    "\n",
    "        # Creamos una instancia de la clase 'NeighborEmbedding' para la representación de vecinos.\n",
    "        # Esto se hace si 'neighbor_embedding' es verdadero, de lo contrario, se establece en 'None'.\n",
    "        self.neighbor_embedding = (\n",
    "            NeighborEmbedding(\n",
    "                hidden_channels, num_rbf, cutoff_lower, cutoff_upper, self.max_z, dtype\n",
    "            ).jittable()\n",
    "            if neighbor_embedding\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Creamos una lista de capas de atención llamada 'attention_layers' utilizando 'nn.ModuleList'.\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = EquivariantMultiHeadAttention(\n",
    "                hidden_channels,\n",
    "                num_rbf,\n",
    "                distance_influence,\n",
    "                num_heads,\n",
    "                act_class,\n",
    "                attn_activation,\n",
    "                cutoff_lower,\n",
    "                cutoff_upper,\n",
    "                dtype,\n",
    "            ).jittable()\n",
    "            self.attention_layers.append(layer)\n",
    "\n",
    "        # Creamos una capa de normalización llamada 'out_norm'.\n",
    "        self.out_norm = nn.LayerNorm(hidden_channels, dtype=dtype)\n",
    "\n",
    "        # Inicializamos los parámetros de la clase llamando al método 'reset_parameters'.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Definimos un método llamado 'reset_parameters'.\n",
    "    def reset_parameters(self):\n",
    "        # Reiniciamos los parámetros de la capa de embedding 'embedding'.\n",
    "        self.embedding.reset_parameters()\n",
    "\n",
    "        # Reiniciamos los parámetros de la expansión de distancia 'distance_expansion'.\n",
    "        self.distance_expansion.reset_parameters()\n",
    "\n",
    "        # Si 'neighbor_embedding' no es 'None', reiniciamos sus parámetros.\n",
    "        if self.neighbor_embedding is not None:\n",
    "            self.neighbor_embedding.reset_parameters()\n",
    "\n",
    "        # Reiniciamos los parámetros de cada capa de atención en 'attention_layers'.\n",
    "        for attn in self.attention_layers:\n",
    "            attn.reset_parameters()\n",
    "\n",
    "        # Reiniciamos los parámetros de la capa de normalización 'out_norm'.\n",
    "        self.out_norm.reset_parameters()\n",
    "\n",
    "\n",
    "    # Definimos un método llamado 'forward', que es utilizado para realizar el pase hacia adelante (forward pass) de la arquitectura.\n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,  # Tipos de átomos\n",
    "        pos: Tensor,  # Coordenadas de los átomos\n",
    "        batch: Tensor,  # Índices de lote\n",
    "        q: Optional[Tensor] = None,  # Opcional: cargas de átomos\n",
    "        s: Optional[Tensor] = None,  # Opcional: masas de átomos\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        # Comenzamos por pasar los tipos de átomos 'z' a través de la capa de embedding 'self.embedding'.\n",
    "        x = self.embedding(z)\n",
    "\n",
    "        # Calculamos las distancias entre los átomos utilizando el módulo 'self.distance'.\n",
    "        edge_index, edge_weight, edge_vec = self.distance(pos, batch)\n",
    "\n",
    "        # Agregamos una aserción (assert) para garantizar que 'edge_vec' no sea 'None'.\n",
    "        # Esto es necesario para convencer a TorchScript de que 'edge_vec' no es opcional.\n",
    "        # 'edge_vec' contiene información direccional sobre las distancias y es crucial para el cálculo de las transformaciones equivariantes.\n",
    "        # This assert must be here to convince TorchScript that edge_vec is not None\n",
    "        # If you remove it TorchScript will complain down below that you cannot use an Optional[Tensor]\n",
    "        assert (\n",
    "            edge_vec is not None\n",
    "        ), \"Distance module did not return directional information\"\n",
    "\n",
    "        # Aplicamos una expansión de distancia a los pesos de los bordes utilizando 'self.distance_expansion'.\n",
    "        edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "        # Creamos una máscara para identificar los bordes válidos.\n",
    "        mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "        # Normalizamos los vectores de borde que no sean cero.\n",
    "        edge_vec[mask] = edge_vec[mask] / torch.norm(edge_vec[mask], dim=1).unsqueeze(1)\n",
    "\n",
    "        # Si 'self.neighbor_embedding' no es 'None', aplicamos la representación de vecinos a los datos 'x'.\n",
    "        if self.neighbor_embedding is not None:\n",
    "            x = self.neighbor_embedding(z, x, edge_index, edge_weight, edge_attr)\n",
    "\n",
    "        # Inicializamos un tensor 'vec' con ceros para almacenar información direccional.\n",
    "        vec = torch.zeros(x.size(0), 3, x.size(1), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # Aplicamos múltiples capas de atención en un bucle.\n",
    "        for attn in self.attention_layers:\n",
    "            dx, dvec = attn(x, vec, edge_index, edge_weight, edge_attr, edge_vec)\n",
    "            x = x + dx\n",
    "            vec = vec + dvec\n",
    "\n",
    "        # Normalizamos la salida 'x' utilizando 'self.out_norm'.\n",
    "        x = self.out_norm(x)\n",
    "\n",
    "        # Devolvemos un conjunto de tensores como resultado de la propagación hacia adelante.\n",
    "        return x, vec, z, pos, batch\n",
    "\n",
    "    # Definimos el método '__repr__' que se utiliza para proporcionar una representación de cadena del objeto de la clase.\n",
    "    def __repr__(self):\n",
    "        # Construimos una cadena que describe los atributos de la instancia de la clase.\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"hidden_channels={self.hidden_channels}, \"\n",
    "            f\"num_layers={self.num_layers}, \"\n",
    "            f\"num_rbf={self.num_rbf}, \"\n",
    "            f\"rbf_type={self.rbf_type}, \"\n",
    "            f\"trainable_rbf={self.trainable_rbf}, \"\n",
    "            f\"activation={self.activation}, \"\n",
    "            f\"attn_activation={self.attn_activation}, \"\n",
    "            f\"neighbor_embedding={self.neighbor_embedding}, \"\n",
    "            f\"num_heads={self.num_heads}, \"\n",
    "            f\"distance_influence={self.distance_influence}, \"\n",
    "            f\"cutoff_lower={self.cutoff_lower}, \"\n",
    "            f\"cutoff_upper={self.cutoff_upper}), \"\n",
    "            f\"dtype={self.dtype}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una nueva clase llamada 'EquivariantMultiHeadAttention' que hereda de 'MessagePassing'.\n",
    "class EquivariantMultiHeadAttention(MessagePassing):\n",
    "    # El método '__init__' es el constructor de la clase y se utiliza para inicializar sus atributos.\n",
    "    def __init(\n",
    "        self,\n",
    "        hidden_channels,  # Número de canales ocultos\n",
    "        num_rbf,  # Número de funciones de base radial (RBF)\n",
    "        distance_influence,  # Influencia de la distancia\n",
    "        num_heads,  # Número de cabezas de atención\n",
    "        activation,  # Función de activación\n",
    "        attn_activation,  # Función de activación de atención\n",
    "        cutoff_lower,  # Valor de corte inferior\n",
    "        cutoff_upper,  # Valor de corte superior\n",
    "        dtype=torch.float32,  # Tipo de datos\n",
    "    ):\n",
    "        # Llamamos al constructor de la clase base 'MessagePassing' con 'aggr=\"add\"' y 'node_dim=0'.\n",
    "        super(EquivariantMultiHeadAttention, self).__init__(aggr=\"add\", node_dim=0)\n",
    "\n",
    "        # Validamos que 'hidden_channels' sea divisible de manera uniforme por 'num_heads'.\n",
    "        assert hidden_channels % num_heads == 0, (\n",
    "            f\"The number of hidden channels ({hidden_channels}) \"\n",
    "            f\"must be evenly divisible by the number of \"\n",
    "            f\"attention heads ({num_heads})\"\n",
    "        )\n",
    "\n",
    "        # Asignamos los parámetros pasados al constructor como atributos de la clase.\n",
    "        self.distance_influence = distance_influence\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.head_dim = hidden_channels // num_heads\n",
    "        self.layernorm = nn.LayerNorm(hidden_channels, dtype=dtype)\n",
    "        self.act = activation()\n",
    "        self.attn_activation = act_class_mapping[attn_activation]()\n",
    "        self.cutoff = CosineCutoff(cutoff_lower, cutoff_upper)\n",
    "\n",
    "        # Definimos proyecciones lineales para 'q', 'k', 'v', y 'o'.\n",
    "        self.q_proj = nn.Linear(hidden_channels, hidden_channels, dtype=dtype)\n",
    "        self.k_proj = nn.Linear(hidden_channels, hidden_channels, dtype=dtype)\n",
    "        self.v_proj = nn.Linear(hidden_channels, hidden_channels * 3, dtype=dtype)\n",
    "        self.o_proj = nn.Linear(hidden_channels, hidden_channels * 3, dtype=dtype)\n",
    "\n",
    "        # Definimos una proyección lineal para 'vec'.\n",
    "        self.vec_proj = nn.Linear(hidden_channels, hidden_channels * 3, bias=False, dtype=dtype)\n",
    "\n",
    "        # Si 'distance_influence' contiene \"keys\" o \"both\", definimos una proyección para 'dk'.\n",
    "        self.dk_proj = None\n",
    "        if distance_influence in [\"keys\", \"both\"]:\n",
    "            self.dk_proj = nn.Linear(num_rbf, hidden_channels, dtype=dtype)\n",
    "\n",
    "        # Si 'distance_influence' contiene \"values\" o \"both\", definimos una proyección para 'dv'.\n",
    "        self.dv_proj = None\n",
    "        if distance_influence in [\"values\", \"both\"]:\n",
    "            self.dv_proj = nn.Linear(num_rbf, hidden_channels * 3, dtype=dtype)\n",
    "\n",
    "        # Inicializamos los parámetros de la clase llamando al método 'reset_parameters'.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Definimos una función llamada 'reset_parameters' utilizada para inicializar los parámetros del modelo.\n",
    "    def reset_parameters(self):\n",
    "        # Reiniciamos los parámetros de la capa de normalización 'layernorm'.\n",
    "        self.layernorm.reset_parameters()\n",
    "\n",
    "        # Inicializamos los parámetros de las proyecciones lineales 'q_proj', 'k_proj', 'v_proj', y 'o_proj'\n",
    "        # utilizando la inicialización Xavier uniforme para los pesos y llenando los sesgos con ceros.\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        self.q_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        self.k_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        self.v_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "        # Inicializamos los parámetros de la proyección lineal 'vec_proj' utilizando Xavier uniforme para los pesos.\n",
    "        nn.init.xavier_uniform_(self.vec_proj.weight)\n",
    "\n",
    "        # Si 'dk_proj' existe (no es 'None'), inicializamos sus parámetros de manera similar a las proyecciones anteriores.\n",
    "        if self.dk_proj:\n",
    "            nn.init.xavier_uniform_(self.dk_proj.weight)\n",
    "            self.dk_proj.bias.data.fill_(0)\n",
    "\n",
    "        # Si 'dv_proj' existe (no es 'None'), inicializamos sus parámetros de manera similar a las proyecciones anteriores.\n",
    "        if self.dv_proj:\n",
    "            nn.init.xavier_uniform_(self.dv_proj.weight)\n",
    "            self.dv_proj.bias.data.fill_(0)\n",
    "\n",
    "    # Definimos un método llamado 'forward' utilizado para realizar el pase hacia adelante de la capa de atención multi-cabeza.\n",
    "    def forward(self, x, vec, edge_index, r_ij, f_ij, d_ij):\n",
    "        # Aplicamos la capa de normalización 'layernorm' a la entrada 'x'.\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        # Proyectamos 'x' en los espacios de consulta, clave y valor ('q', 'k', 'v') y reformateamos para la atención multi-cabeza.\n",
    "        q = self.q_proj(x).reshape(-1, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(-1, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(-1, self.num_heads, self.head_dim * 3)\n",
    "\n",
    "        # Dividimos 'vec_proj' en tres partes y realizamos una operación de producto escalar para obtener 'vec_dot'.\n",
    "        vec1, vec2, vec3 = torch.split(self.vec_proj(vec), self.hidden_channels, dim=-1)\n",
    "        vec = vec.reshape(-1, 3, self.num_heads, self.head_dim)\n",
    "        vec_dot = (vec1 * vec2).sum(dim=1)\n",
    "\n",
    "        # Proyectamos 'f_ij' en 'dk' (claves) y 'dv' (valores) si las proyecciones existen.\n",
    "        dk = (\n",
    "            self.act(self.dk_proj(f_ij)).reshape(-1, self.num_heads, self.head_dim)\n",
    "            if self.dk_proj is not None\n",
    "            else None\n",
    "        )\n",
    "        dv = (\n",
    "            self.act(self.dv_proj(f_ij)).reshape(-1, self.num_heads, self.head_dim * 3)\n",
    "            if self.dv_proj is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Llamamos al método 'propagate' para realizar la atención multi-cabeza y obtener resultados.\n",
    "        # propagate_type: (q: Tensor, k: Tensor, v: Tensor, vec: Tensor, dk: Tensor, dv: Tensor, r_ij: Tensor, d_ij: Tensor)\n",
    "        x, vec = self.propagate(\n",
    "            edge_index,\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            vec=vec,\n",
    "            dk=dk,\n",
    "            dv=dv,\n",
    "            r_ij=r_ij,\n",
    "            d_ij=d_ij,\n",
    "            size=None,\n",
    "        )\n",
    "        # Reformateamos los resultados para que tengan la forma adecuada.\n",
    "        x = x.reshape(-1, self.hidden_channels)\n",
    "        vec = vec.reshape(-1, 3, self.hidden_channels)\n",
    "\n",
    "        # Dividimos la proyección de salida 'o_proj' en tres partes y calculamos 'dx' y 'dvec'.\n",
    "        o1, o2, o3 = torch.split(self.o_proj(x), self.hidden_channels, dim=1)\n",
    "        dx = vec_dot * o2 + o3\n",
    "        dvec = vec3 * o1.unsqueeze(1) + vec\n",
    "        # Devolvemos 'dx' y 'dvec' como resultados de la atención multi-cabeza.\n",
    "        return dx, dvec\n",
    "\n",
    "    def message(self, q_i, k_j, v_j, vec_j, dk, dv, r_ij, d_ij):\n",
    "        # attention mechanism\n",
    "        if dk is None:\n",
    "            # Si 'dk' no está definido (influencia de la distancia en las claves no está activa),\n",
    "            # calculamos la atención como el producto escalar entre 'q_i' y 'k_j'.\n",
    "            attn = (q_i * k_j).sum(dim=-1)\n",
    "        else:\n",
    "            # Si 'dk' está definido (influencia de la distancia en las claves está activa),\n",
    "            # calculamos la atención como el producto escalar entre 'q_i', 'k_j' y 'dk'.\n",
    "            attn = (q_i * k_j * dk).sum(dim=-1)\n",
    "\n",
    "        # attention activation function\n",
    "        attn = self.attn_activation(attn) * self.cutoff(r_ij).unsqueeze(1)\n",
    "\n",
    "        # value pathway\n",
    "        if dv is not None:\n",
    "            # Si 'dv' está definido (influencia de la distancia en los valores está activa),\n",
    "            # aplicamos 'dv' a 'v_j'.\n",
    "            v_j = v_j * dv\n",
    "        # Dividimos 'v_j' en tres partes ('x', 'vec1', 'vec2') de acuerdo a la dimensión de cabezas.\n",
    "        x, vec1, vec2 = torch.split(v_j, self.head_dim, dim=2)\n",
    "\n",
    "        # update scalar features\n",
    "        x = x * attn.unsqueeze(2)\n",
    "        # update vector features\n",
    "        vec = vec_j * vec1.unsqueeze(1) + vec2.unsqueeze(1) * d_ij.unsqueeze(\n",
    "            2\n",
    "        ).unsqueeze(3)\n",
    "        # Devolvemos las características actualizadas 'x' y 'vec'.\n",
    "        return x, vec\n",
    "\n",
    "    # Definimos un método llamado 'aggregate' que se utiliza para realizar la agregación de características.\n",
    "    def aggregate(\n",
    "        self,\n",
    "        features: Tuple[torch.Tensor, torch.Tensor],\n",
    "        index: torch.Tensor,\n",
    "        ptr: Optional[torch.Tensor],\n",
    "        dim_size: Optional[int],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Desempaquetamos las características en 'x' y 'vec'.\n",
    "        x, vec = features\n",
    "        # Utilizamos la función 'scatter' para agregar 'x' y 'vec' de acuerdo al índice 'index'.\n",
    "        # Esto permite agrupar las características correspondientes a los mismos nodos.\n",
    "        x = scatter(x, index, dim=self.node_dim, dim_size=dim_size)\n",
    "        vec = scatter(vec, index, dim=self.node_dim, dim_size=dim_size)\n",
    "        # Devolvemos las características agregadas 'x' y 'vec'.\n",
    "        return x, vec\n",
    "\n",
    "    # Definimos un método llamado 'update' que se utiliza para actualizar las características después de la agregación.\n",
    "    def update(\n",
    "        self, inputs: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # En esta implementación actual, la función no realiza ninguna operación.\n",
    "        # Simplemente devuelve las mismas características de entrada.\n",
    "\n",
    "        # Devolvemos las mismas características de entrada 'inputs'.\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envoltorio del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importamos el módulo 're' para trabajar con expresiones regulares.\n",
    "from typing import Optional, List, Tuple, Dict # Importamos tipos de Python para anotaciones de tipos.\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from torch import nn, Tensor\n",
    "from torch_scatter import scatter\n",
    "from pytorch_lightning.utilities import rank_zero_warn# Importamos una función 'rank_zero_warn' desde el módulo 'rank_zero_warn' en 'pytorch_lightning.utilities'.\n",
    "#from torchmdnet.models import output_modules\n",
    "#from torchmdnet.models.wrappers import AtomFilter\n",
    "#from torchmdnet.models.utils import dtype_mapping\n",
    "#from torchmdnet import priors\n",
    "import warnings # Importamos el módulo 'warnings' para gestionar advertencias en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, prior_model=None, mean=None, std=None):\n",
    "    \"\"\"Create a model from the given arguments.\n",
    "    See :func:`get_args` in scripts/train.py for a description of the arguments.\n",
    "    Parameters\n",
    "    ----------\n",
    "        args (dict): Arguments for the model.\n",
    "        prior_model (nn.Module, optional): Prior model to use. Defaults to None.\n",
    "        mean (torch.Tensor, optional): Mean of the training data. Defaults to None.\n",
    "        std (torch.Tensor, optional): Standard deviation of the training data. Defaults to None.\n",
    "    Returns\n",
    "    -------\n",
    "        nn.Module: An instance of the TorchMD_Net model.\n",
    "    \"\"\"\n",
    "    # Mapeamos el tipo de datos (dtype) según la precisión especificada en los argumentos.\n",
    "    dtype = dtype_mapping[args[\"precision\"]]\n",
    "\n",
    "    # Definimos un diccionario con argumentos compartidos necesarios para crear el modelo.\n",
    "    shared_args = dict(\n",
    "        hidden_channels=args[\"embedding_dimension\"],\n",
    "        num_layers=args[\"num_layers\"],\n",
    "        num_rbf=args[\"num_rbf\"],\n",
    "        rbf_type=args[\"rbf_type\"],\n",
    "        trainable_rbf=args[\"trainable_rbf\"],\n",
    "        activation=args[\"activation\"],\n",
    "        cutoff_lower=args[\"cutoff_lower\"],\n",
    "        cutoff_upper=args[\"cutoff_upper\"],\n",
    "        max_z=args[\"max_z\"],\n",
    "        max_num_neighbors=args[\"max_num_neighbors\"],\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    # representation network\n",
    "    if args[\"model\"] == \"graph-network\":\n",
    "        from torchmdnet.models.torchmd_gn import TorchMD_GN\n",
    "\n",
    "        is_equivariant = False\n",
    "        representation_model = TorchMD_GN(\n",
    "            num_filters=args[\"embedding_dimension\"],\n",
    "            aggr=args[\"aggr\"],\n",
    "            neighbor_embedding=args[\"neighbor_embedding\"],\n",
    "            **shared_args\n",
    "        )\n",
    "    elif args[\"model\"] == \"transformer\":\n",
    "        from torchmdnet.models.torchmd_t import TorchMD_T\n",
    "\n",
    "        is_equivariant = False\n",
    "        representation_model = TorchMD_T(\n",
    "            attn_activation=args[\"attn_activation\"],\n",
    "            num_heads=args[\"num_heads\"],\n",
    "            distance_influence=args[\"distance_influence\"],\n",
    "            neighbor_embedding=args[\"neighbor_embedding\"],\n",
    "            **shared_args,\n",
    "        )\n",
    "    elif args[\"model\"] == \"equivariant-transformer\":\n",
    "        from torchmdnet.models.torchmd_et import TorchMD_ET\n",
    "\n",
    "        is_equivariant = True\n",
    "        representation_model = TorchMD_ET(\n",
    "            attn_activation=args[\"attn_activation\"],\n",
    "            num_heads=args[\"num_heads\"],\n",
    "            distance_influence=args[\"distance_influence\"],\n",
    "            neighbor_embedding=args[\"neighbor_embedding\"],\n",
    "            **shared_args,\n",
    "        )\n",
    "    elif args[\"model\"] == \"tensornet\":\n",
    "        from torchmdnet.models.tensornet import TensorNet\n",
    "\t# Setting is_equivariant to False to enforce the use of Scalar output module instead of EquivariantScalar\n",
    "        is_equivariant = False\n",
    "        representation_model = TensorNet(\n",
    "\t    equivariance_invariance_group=args[\"equivariance_invariance_group\"],\n",
    "            **shared_args,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown architecture: {args[\"model\"]}')\n",
    "\n",
    "    # atom filter\n",
    "    if not args[\"derivative\"] and args[\"atom_filter\"] > -1:\n",
    "        representation_model = AtomFilter(representation_model, args[\"atom_filter\"])\n",
    "    elif args[\"atom_filter\"] > -1:\n",
    "        raise ValueError(\"Derivative and atom filter can't be used together\")\n",
    "\n",
    "    # prior model\n",
    "    if args[\"prior_model\"] and prior_model is None:\n",
    "        # instantiate prior model if it was not passed to create_model (i.e. when loading a model)\n",
    "        prior_model = create_prior_models(args)\n",
    "\n",
    "    # create output network\n",
    "    output_prefix = \"Equivariant\" if is_equivariant else \"\"\n",
    "#    output_model = getattr(output_modules, output_prefix + args[\"output_model\"])(\n",
    "    output_model = EquivariantScalar(\n",
    "        args[\"embedding_dimension\"],\n",
    "        activation=args[\"activation\"],\n",
    "        reduce_op=args[\"reduce_op\"],\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # combine representation and output network\n",
    "    model = TorchMD_Net(\n",
    "        representation_model,\n",
    "        output_model,\n",
    "        prior_model=prior_model,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        derivative=args[\"derivative\"],\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, args=None, device=\"cpu\", **kwargs):\n",
    "    # Cargamos el estado del modelo desde el archivo\n",
    "    ckpt = torch.load(filepath, map_location=\"cpu\")\n",
    "    # Si no se proporcionaron los argumentos 'args', los obtenemos del checkpoint\n",
    "    if args is None:\n",
    "        args = ckpt[\"hyper_parameters\"]\n",
    "\n",
    "    # Actualizamos los argumentos con los valores proporcionados en kwargs\n",
    "    for key, value in kwargs.items():\n",
    "        if not key in args:\n",
    "            warnings.warn(f\"Unknown hyperparameter: {key}={value}\")\n",
    "        args[key] = value\n",
    "\n",
    "    # Creamos un modelo nuevo utilizando los argumentos\n",
    "    model = create_model(args)\n",
    "\n",
    "    # Preparamos el estado del modelo cargado\n",
    "    state_dict = {re.sub(r\"^model\\.\", \"\", k): v for k, v in ckpt[\"state_dict\"].items()}\n",
    "    # The following are for backward compatibility with models created when atomref was\n",
    "    # the only supported prior.\n",
    "    if 'prior_model.initial_atomref' in state_dict:\n",
    "        state_dict['prior_model.0.initial_atomref'] = state_dict['prior_model.initial_atomref']\n",
    "        del state_dict['prior_model.initial_atomref']\n",
    "    if 'prior_model.atomref.weight' in state_dict:\n",
    "        state_dict['prior_model.0.atomref.weight'] = state_dict['prior_model.atomref.weight']\n",
    "        del state_dict['prior_model.atomref.weight']\n",
    "\n",
    "    # Cargamos el estado del modelo en el modelo creado\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Transferimos el modelo al dispositivo especificado (por defecto, \"cpu\")\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prior_models(args, dataset=None):\n",
    "    \"\"\"Parse the prior_model configuration option and create the prior models.\"\"\"\n",
    "    prior_models = [] # Lista para almacenar los modelos previos creados\n",
    "    if args['prior_model']:\n",
    "        prior_model = args['prior_model']\n",
    "        prior_names = []\n",
    "        prior_args = []\n",
    "        # Si prior_model es una lista, iteramos sobre sus elementos\n",
    "        if not isinstance(prior_model, list):\n",
    "            prior_model = [prior_model]\n",
    "        # Procesamos cada elemento de prior_model\n",
    "        for prior in prior_model:\n",
    "            if isinstance(prior, dict):\n",
    "                # Si el elemento es un diccionario, se espera que contenga el nombre del modelo y sus argumentos\n",
    "                for key, value in prior.items():\n",
    "                    prior_names.append(key)\n",
    "                    if value is None:\n",
    "                        prior_args.append({})\n",
    "                    else:\n",
    "                        prior_args.append(value)\n",
    "            else:\n",
    "                # Si el elemento no es un diccionario, se considera que es el nombre del modelo sin argumentos\n",
    "                prior_names.append(prior)\n",
    "                prior_args.append({})\n",
    "\n",
    "        # Si se proporcionan argumentos específicos en 'prior_args', los utilizamos en lugar de los argumentos encontrados\n",
    "        if 'prior_args' in args:\n",
    "            prior_args = args['prior_args']\n",
    "            if not isinstance(prior_args, list):\n",
    "                prior_args = [prior_args]\n",
    "\n",
    "        # Creamos los modelos previos\n",
    "        for name, arg in zip(prior_names, prior_args):\n",
    "#            assert hasattr(priors, name), (\n",
    "#                f\"Unknown prior model {name}. \"\n",
    "#                f\"Available models are {', '.join(priors.__all__)}\"\n",
    "#            )\n",
    "            # initialize the prior model\n",
    "#            prior_models.append(getattr(priors, name)(dataset=dataset, **arg))\n",
    "            prior_models.append(Atomref(dataset=dataset, **arg))\n",
    "    return prior_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchMD_Net(nn.Module):\n",
    "    \"\"\"The  TorchMD_Net class  combines a  given representation  model\n",
    "    (such as  the equivariant transformer),  an output model  (such as\n",
    "    the scalar output  module) and a prior model (such  as the atomref\n",
    "    prior), producing a  Module that takes as input a  series of atoms\n",
    "    features  and  outputs  a  scalar   value  (i.e  energy  for  each\n",
    "    batch/molecule) and,  derivative is True, the  negative of  its derivative\n",
    "    with respect to the positions (i.e forces for each atom).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        representation_model,\n",
    "        output_model,\n",
    "        prior_model=None,\n",
    "        mean=None,\n",
    "        std=None,\n",
    "        derivative=False,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        super(TorchMD_Net, self).__init__()\n",
    "        self.representation_model = representation_model.to(dtype=dtype)\n",
    "        self.output_model = output_model.to(dtype=dtype)\n",
    "\n",
    "        if not output_model.allow_prior_model and prior_model is not None:\n",
    "            prior_model = None\n",
    "            rank_zero_warn(\n",
    "                (\n",
    "                    \"Prior model was given but the output model does \"\n",
    "                    \"not allow prior models. Dropping the prior model.\"\n",
    "                )\n",
    "            )\n",
    "#        if isinstance(prior_model, priors.base.BasePrior):\n",
    "#            prior_model = [prior_model]\n",
    "        self.prior_model = None if prior_model is None else torch.nn.ModuleList(prior_model).to(dtype=dtype)\n",
    "\n",
    "        self.derivative = derivative\n",
    "\n",
    "        mean = torch.scalar_tensor(0) if mean is None else mean\n",
    "        self.register_buffer(\"mean\", mean.to(dtype=dtype))\n",
    "        std = torch.scalar_tensor(1) if std is None else std\n",
    "        self.register_buffer(\"std\", std.to(dtype=dtype))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.representation_model.reset_parameters()\n",
    "        self.output_model.reset_parameters()\n",
    "        if self.prior_model is not None:\n",
    "            for prior in self.prior_model:\n",
    "                prior.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        pos: Tensor,\n",
    "        batch: Optional[Tensor] = None,\n",
    "        q: Optional[Tensor] = None,\n",
    "        s: Optional[Tensor] = None,\n",
    "        extra_args: Optional[Dict[str, Tensor]] = None\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Compute the output of the model.\n",
    "        Args:\n",
    "            z (Tensor): Atomic numbers of the atoms in the molecule. Shape (N,).\n",
    "            pos (Tensor): Atomic positions in the molecule. Shape (N, 3).\n",
    "            batch (Tensor, optional): Batch indices for the atoms in the molecule. Shape (N,).\n",
    "            q (Tensor, optional): Atomic charges in the molecule. Shape (N,).\n",
    "            s (Tensor, optional): Atomic spins in the molecule. Shape (N,).\n",
    "            extra_args (Dict[str, Tensor], optional): Extra arguments to pass to the prior model.\n",
    "        \"\"\"\n",
    "\n",
    "        assert z.dim() == 1 and z.dtype == torch.long\n",
    "        batch = torch.zeros_like(z) if batch is None else batch\n",
    "\n",
    "        if self.derivative:\n",
    "            pos.requires_grad_(True)\n",
    "\n",
    "        # run the potentially wrapped representation model\n",
    "        x, v, z, pos, batch = self.representation_model(z, pos, batch, q=q, s=s)\n",
    "\n",
    "        # apply the output network\n",
    "        x = self.output_model.pre_reduce(x, v, z, pos, batch)\n",
    "\n",
    "        # scale by data standard deviation\n",
    "        if self.std is not None:\n",
    "            x = x * self.std\n",
    "\n",
    "        # apply atom-wise prior model\n",
    "        if self.prior_model is not None:\n",
    "            for prior in self.prior_model:\n",
    "                x = prior.pre_reduce(x, z, pos, batch, extra_args)\n",
    "\n",
    "        # aggregate atoms\n",
    "        x = self.output_model.reduce(x, batch)\n",
    "\n",
    "        # shift by data mean\n",
    "        if self.mean is not None:\n",
    "            x = x + self.mean\n",
    "\n",
    "        # apply output model after reduction\n",
    "        y = self.output_model.post_reduce(x)\n",
    "\n",
    "        # apply molecular-wise prior model\n",
    "        if self.prior_model is not None:\n",
    "            for prior in self.prior_model:\n",
    "                y = prior.post_reduce(y, z, pos, batch, extra_args)\n",
    "\n",
    "        # compute gradients with respect to coordinates\n",
    "        if self.derivative:\n",
    "            grad_outputs: List[Optional[torch.Tensor]] = [torch.ones_like(y)]\n",
    "            dy = grad(\n",
    "                [y],\n",
    "                [pos],\n",
    "                grad_outputs=grad_outputs,\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0]\n",
    "            if dy is None:\n",
    "                raise RuntimeError(\"Autograd returned None for the force prediction.\")\n",
    "\n",
    "            return y, -dy\n",
    "        # TODO: return only `out` once Union typing works with TorchScript (https://github.com/pytorch/pytorch/pull/53180)\n",
    "        return y, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulos de la compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW #se utiliza para optimizar modelos durante el entrenamiento\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau #un programador de tasa de aprendizaje que ajusta la tasa de aprendizaje en función de las métricas de entrenamiento\n",
    "from torch.nn.functional import mse_loss, l1_loss # mse_loss (pérdida de error cuadrático medio) y l1_loss (pérdida L1)\n",
    "from torch import Tensor #tipo de dato 'Tensor' de PyTorch\n",
    "from typing import Optional, Dict, Tuple #anotaciones de tipo opcionales, diccionarios y tuplas del módulo 'typing'\n",
    "\n",
    "from pytorch_lightning import LightningModule #clase 'LightningModule' de PyTorch Lightning, que se utiliza para crear módulos de entrenamiento\n",
    "# from torchmdnet.models.model import create_model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase LNNP que hereda de LightningModule, lo que permite el uso de PyTorch Lightning para entrenar y evaluar modelos.\n",
    "class LNNP(LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning wrapper for the Neural Network Potentials in TorchMD-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, hparams, prior_model=None, mean=None, std=None):\n",
    "        # Llamamos al constructor de la clase padre (LightningModule) utilizando 'super'.\n",
    "        super(LNNP, self).__init__()\n",
    "        \n",
    "        # Verificamos si 'charge' y 'spin' están definidos en los hiperparámetros; si no, los establecemos como 'False'.\n",
    "        if \"charge\" not in hparams:\n",
    "            hparams[\"charge\"] = False\n",
    "        if \"spin\" not in hparams:\n",
    "            hparams[\"spin\"] = False\n",
    "\n",
    "        # Guardamos los hiperparámetros en el modelo para que estén disponibles en todo momento.\n",
    "        self.save_hyperparameters(hparams)\n",
    "    \n",
    "        # Si se especifica un modelo previamente entrenado, lo cargamos utilizando 'load_model'; de lo contrario, creamos un nuevo modelo utilizando 'create_model'.\n",
    "        if self.hparams.load_model:\n",
    "            self.model = load_model(self.hparams.load_model, args=self.hparams)\n",
    "        else:\n",
    "            self.model = create_model(self.hparams, prior_model, mean, std)\n",
    "\n",
    "        # Inicializamos el suavizado exponencial (exponential moving average, EMA).\n",
    "        # initialize exponential smoothing\n",
    "        self.ema = None\n",
    "        self._reset_ema_dict() # Llamamos a la función para configurar la EMA.\n",
    "\n",
    "        # Inicializamos una colección de pérdidas (losses).\n",
    "        # initialize loss collection\n",
    "        self.losses = None\n",
    "        self._reset_losses_dict()    # Llamamos a la función para configurar la colección de pérdidas.\n",
    "\n",
    "        # Inicializamos una lista para almacenar las pérdidas de validación de la suma de errores cuadráticos ('val_sqr_e').\n",
    "        self.losses[\"val_sqr_e\"] = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Se configura el optimizador AdamW para actualizar los parámetros del modelo durante el entrenamiento.\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(), # Parámetros del modelo a ser optimizados.\n",
    "            lr=self.hparams.lr,  # Tasa de aprendizaje, obtenida de los hiperparámetros.\n",
    "            weight_decay=self.hparams.weight_decay, # Término de decaimiento de peso (regularización L2).\n",
    "        )\n",
    "        # Se configura un programador de la tasa de aprendizaje (scheduler) ReduceLROnPlateau para ajustar la tasa de aprendizaje durante el entrenamiento.\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,    # Optimizador al que se aplica el programador.\n",
    "            \"min\",    # Modo de reducción: \"min\" significa reducir la tasa de aprendizaje cuando la métrica disminuye.\n",
    "            factor=self.hparams.lr_factor,    # Factor de reducción de la tasa de aprendizaje.\n",
    "            patience=self.hparams.lr_patience,    # Paciencia: número de épocas sin mejoras antes de reducir la tasa de aprendizaje.\n",
    "            min_lr=self.hparams.lr_min,    # Tasa de aprendizaje mínima permitida.\n",
    "        )\n",
    "        # Se configura un diccionario 'lr_scheduler' que contiene información sobre el programador de la tasa de aprendizaje.\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": scheduler,    # El programador previamente configurado.\n",
    "            \"monitor\": getattr(self.hparams, \"lr_metric\", \"val_loss\"),   # Métrica a monitorizar para ajustar la tasa de aprendizaje.\n",
    "            \"interval\": \"epoch\",  # Intervalo de ajuste de la tasa de aprendizaje por época.\n",
    "            \"frequency\": 1, # Frecuencia de ajuste de la tasa de aprendizaje (cada época en este caso).\n",
    "        }\n",
    "        # Se devuelve una lista con el optimizador y una lista con el programador de tasa de aprendizaje.\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def forward(self,\n",
    "                z: Tensor,  # Tensor de características de átomos.\n",
    "                pos: Tensor, # Tensor de coordenadas tridimensionales de átomos.\n",
    "                batch: Optional[Tensor] = None,  # Tensor de índices de lotes (opcional).\n",
    "                q: Optional[Tensor] = None,  # Tensor de cargas atómicas (opcional).\n",
    "                s: Optional[Tensor] = None,  # Tensor de espines (opcional).\n",
    "                extra_args: Optional[Dict[str, Tensor]] = None  # Argumentos adicionales (opcional).\n",
    "                ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # El método simplemente redirige la llamada al método 'forward' del modelo interno 'self.model'.\n",
    "        return self.model(z, pos, batch=batch, q=q, s=s, extra_args=extra_args)\n",
    "\n",
    "    # Método para una etapa de entrenamiento de un modelo de aprendizaje automático.\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Devuelve el resultado de la función 'step' con los parámetros de lote, la función de pérdida MSE y la etiqueta \"train\".\n",
    "        return self.step(batch, mse_loss, \"train\")\n",
    "\n",
    "    # Método para una etapa de validación de un modelo de aprendizaje automático.\n",
    "    def validation_step(self, batch, batch_idx, *args):\n",
    "        # Verifica si hay argumentos adicionales y si el primer argumento es cero o no se proporciona ningún argumento.\n",
    "        if len(args) == 0 or (len(args) > 0 and args[0] == 0):\n",
    "            # Paso de validación. Devuelve el resultado de la función 'step' con los parámetros de lote, la función de pérdida MSE y la etiqueta \"val\".\n",
    "            # validation step\n",
    "            return self.step(batch, mse_loss, \"val\")\n",
    "        # Paso de prueba. Devuelve el resultado de la función 'step' con los parámetros de lote, la función de pérdida L1 y la etiqueta \"test\".\n",
    "        # test step\n",
    "        return self.step(batch, l1_loss, \"test\")\n",
    "    # Método para una etapa de prueba de un modelo de aprendizaje automático.\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Devuelve el resultado de la función 'step' con los parámetros de lote, la función de pérdida L1 y la etiqueta \"test\".\n",
    "        return self.step(batch, l1_loss, \"test\")\n",
    "\n",
    "    # Método principal que se utiliza para realizar un paso de procesamiento en una etapa específica (entrenamiento, validación o prueba).\n",
    "    # Recibe un lote de datos (batch), una función de pérdida (loss_fn) y una etiqueta de etapa (stage).\n",
    "    def step(self, batch, loss_fn, stage):\n",
    "        # Habilita o deshabilita el cálculo de gradientes según si estamos en la etapa de entrenamiento o si se requieren derivadas (self.hparams.derivative).\n",
    "        with torch.set_grad_enabled(stage == \"train\" or self.hparams.derivative):\n",
    "            # Crea un diccionario 'extra_args' a partir de los atributos del lote 'batch'.\n",
    "            # Luego, elimina ciertos elementos ('y', 'neg_dy', 'z', 'pos', 'batch', 'q', 's') de 'extra_args'.\n",
    "            extra_args = batch.to_dict()\n",
    "            for a in ('y', 'neg_dy', 'z', 'pos', 'batch', 'q', 's'):\n",
    "                if a in extra_args:\n",
    "                    del extra_args[a]\n",
    "            # TODO: the model doesn't necessarily need to return a derivative once\n",
    "            # Union typing works under TorchScript (https://github.com/pytorch/pytorch/pull/53180)\n",
    "\n",
    "            # Realiza la inferencia del modelo utilizando los datos del lote y obtiene las predicciones 'y' y las derivadas negativas 'neg_dy'.            \n",
    "            y, neg_dy = self(\n",
    "                batch.z,\n",
    "                batch.pos,\n",
    "                batch=batch.batch,\n",
    "                q=batch.q if self.hparams.charge else None,\n",
    "                s=batch.s if self.hparams.spin else None,\n",
    "                extra_args=extra_args\n",
    "            )\n",
    "\n",
    "        # Inicializa las variables para las pérdidas 'loss_y' y 'loss_neg_dy'.\n",
    "        loss_y, loss_neg_dy = 0, 0\n",
    "        # Si se requieren derivadas (self.hparams.derivative) y no se proporcionó la etiqueta \"y\" en el lote.\n",
    "        if self.hparams.derivative:\n",
    "            if \"y\" not in batch:\n",
    "                # \"use\" both outputs of the model's forward function but discard the first\n",
    "                # to only use the negative derivative and avoid 'Expected to have finished reduction\n",
    "                # in the prior iteration before starting a new one.', which otherwise get's\n",
    "                # thrown because of setting 'find_unused_parameters=False' in the DDPPlugin\n",
    "                # Suma los valores de 'y' pero los descarta, lo que evita ciertos errores de reducción en paralelo.\n",
    "                neg_dy = neg_dy + y.sum() * 0\n",
    "\n",
    "            # Calcula la pérdida de derivadas negativas ('loss_neg_dy') utilizando la función de pérdida proporcionada.\n",
    "            # negative derivative loss\n",
    "            loss_neg_dy = loss_fn(neg_dy, batch.neg_dy)\n",
    "\n",
    "            # Realiza un suavizado exponencial de la pérdida de derivadas negativas si se encuentra en etapa de entrenamiento o validación.\n",
    "            if stage in [\"train\", \"val\"] and self.hparams.ema_alpha_neg_dy < 1:\n",
    "                if self.ema[stage + \"_neg_dy\"] is None:\n",
    "                    self.ema[stage + \"_neg_dy\"] = loss_neg_dy.detach()\n",
    "                # apply exponential smoothing over batches to neg_dy\n",
    "                loss_neg_dy = (\n",
    "                    self.hparams.ema_alpha_neg_dy * loss_neg_dy\n",
    "                    + (1 - self.hparams.ema_alpha_neg_dy) * self.ema[stage + \"_neg_dy\"]\n",
    "                )\n",
    "                self.ema[stage + \"_neg_dy\"] = loss_neg_dy.detach()\n",
    "\n",
    "            # Agrega la pérdida de derivadas negativas al registro de pérdidas si su peso es mayor que cero.\n",
    "            if self.hparams.neg_dy_weight > 0:\n",
    "                self.losses[stage + \"_neg_dy\"].append(loss_neg_dy.detach())\n",
    "\n",
    "        # Si la etiqueta \"y\" está presente en el lote.\n",
    "        if \"y\" in batch:\n",
    "            if batch.y.ndim == 1:\n",
    "                batch.y = batch.y.unsqueeze(1)\n",
    "\n",
    "            # Calcula el error cuadrático medio y la diferencia de valor absoluto entre las predicciones y las etiquetas reales.\n",
    "            # Calcula la pérdida 'loss_y' utilizando la función de pérdida proporcionada.\n",
    "            # y loss\n",
    "            loss_y = loss_fn(y, batch.y)\n",
    "            squared_errors = mse_loss(y, batch.y)\n",
    "            absolute_value_difference = l1_loss(y, batch.y)\n",
    "\n",
    "            # Realiza un suavizado exponencial de la pérdida 'loss_y' si se encuentra en etapa de entrenamiento o validación.\n",
    "            if stage in [\"train\", \"val\"] and self.hparams.ema_alpha_y < 1:\n",
    "                if self.ema[stage + \"_y\"] is None:\n",
    "                    self.ema[stage + \"_y\"] = loss_y.detach()\n",
    "                # apply exponential smoothing over batches to y\n",
    "                loss_y = (\n",
    "                    self.hparams.ema_alpha_y * loss_y\n",
    "                    + (1 - self.hparams.ema_alpha_y) * self.ema[stage + \"_y\"]\n",
    "                )\n",
    "                self.ema[stage + \"_y\"] = loss_y.detach()\n",
    "\n",
    "            # Agrega la pérdida 'loss_y' al registro de pérdidas si su peso es mayor que cero.\n",
    "            if self.hparams.y_weight > 0:\n",
    "                self.losses[stage + \"_y\"].append(loss_y.detach())\n",
    "\n",
    "        # Calcula la pérdida total como una combinación ponderada de 'loss_y' y 'loss_neg_dy'.\n",
    "        # total loss\n",
    "        loss = loss_y * self.hparams.y_weight + loss_neg_dy * self.hparams.neg_dy_weight\n",
    "\n",
    "        # Agrega la pérdida total al registro de pérdidas para la etapa actual.\n",
    "        self.losses[stage].append(loss.detach())\n",
    "        self.losses[stage + \"_sqr_e\"].append(squared_errors.detach())\n",
    "        self.losses[stage + \"_avd_e\"].append(absolute_value_difference.detach())\n",
    "        # Devuelve la pérdida total calculada.\n",
    "        return loss\n",
    "\n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        # Obtiene el optimizador del diccionario de argumentos 'kwargs' si está presente, de lo contrario, toma el tercer argumento en 'args'.\n",
    "        optimizer = kwargs[\"optimizer\"] if \"optimizer\" in kwargs else args[2]\n",
    "        # Verifica si el paso global actual del entrenador es menor que el número de pasos de calentamiento de la tasa de aprendizaje (lr_warmup_steps).\n",
    "        if self.trainer.global_step < self.hparams.lr_warmup_steps:\n",
    "            # Calcula un factor de escala 'lr_scale' para la tasa de aprendizaje basado en el progreso del paso global actual.\n",
    "            lr_scale = min(\n",
    "                1.0,\n",
    "                float(self.trainer.global_step + 1)\n",
    "                / float(self.hparams.lr_warmup_steps),\n",
    "            )\n",
    "            # Ajusta la tasa de aprendizaje en cada grupo de parámetros del optimizador.\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr_scale * self.hparams.lr\n",
    "        # Llama al método 'optimizer_step' de la clase base (superclase) para realizar la actualización estándar del optimizador.\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        # Limpia los gradientes acumulados en el optimizador para evitar que se acumulen gradientes de lotes anteriores.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # Obtiene una referencia al DataModule (dm) desde el entrenador (trainer).\n",
    "        dm = self.trainer.datamodule\n",
    "        # Verifica si el DataModule tiene un conjunto de datos de prueba ('test_dataset') y si tiene al menos un elemento.\n",
    "        if hasattr(dm, \"test_dataset\") and len(dm.test_dataset) > 0:\n",
    "            # Determina si se debe restablecer el conjunto de validación antes y después de la época de prueba.\n",
    "            should_reset = (\n",
    "                self.current_epoch % self.hparams.test_interval == 0\n",
    "                or (self.current_epoch + 1) % self.hparams.test_interval == 0\n",
    "            )\n",
    "            # Si se determina que se debe restablecer el conjunto de validación, lo hace.\n",
    "            if should_reset:\n",
    "                # reset validation dataloaders before and after testing epoch, which is faster\n",
    "                # than skipping test validation steps by returning None\n",
    "                self.trainer.reset_val_dataloader(self)\n",
    "\n",
    "    # Método llamado al final de cada época de validación.\n",
    "    # Recibe una lista de salidas de los pasos de validación de la época (validation_step_outputs).\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # Verifica si no se está realizando una comprobación de cordura (sanity_checking) del entrenador.\n",
    "        if not self.trainer.sanity_checking:\n",
    "            # Filtra y selecciona las pérdidas de entrenamiento (train_avd_e) que no contienen valores NaN o infinitos.\n",
    "            valid_losses_train = [loss for loss in self.losses[\"train_avd_e\"] if not torch.isnan(loss).any()]\n",
    "            valid_losses_train = [loss for loss in valid_losses_train if not torch.isinf(loss).any() and torch.max(loss) < float(\"inf\")]\n",
    "            # Calcula la desviación estándar de las pérdidas de entrenamiento.            \n",
    "            std_train = torch.std(torch.stack(valid_losses_train))\n",
    "\n",
    "            # Filtra y selecciona las pérdidas de validación (val_avd_e) que no contienen valores NaN o infinitos.\n",
    "            valid_losses_val = [loss for loss in self.losses[\"val_avd_e\"] if not torch.isnan(loss).any()]\n",
    "            valid_losses_val = [loss for loss in valid_losses_val if not torch.isinf(loss).any() and torch.max(loss) < float(\"inf\")]\n",
    "            # Calcula la desviación estándar de las pérdidas de validación.\n",
    "            std_val = torch.std(torch.stack(valid_losses_val))\n",
    "\n",
    "            # Construye un diccionario que contiene diversas métricas y estadísticas calculadas.\n",
    "            # construct dict of logged metrics\n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch), # Número de la época actual.\n",
    "                \"lr\": self.trainer.optimizers[0].param_groups[0][\"lr\"], # Tasa de aprendizaje.\n",
    "                \"train_loss\": torch.stack(self.losses[\"train\"]).mean(), # Pérdida promedio de entrenamiento.\n",
    "                \"val_loss\": torch.stack(self.losses[\"val\"]).mean(), # Pérdida promedio de validación.\n",
    "                \"std_train\":std_train, # Desviación estándar de las pérdidas de entrenamiento.\n",
    "                \"mse_train\":torch.stack(self.losses[\"train_sqr_e\"]).mean(), # Error cuadrático medio de entrenamiento.\n",
    "                \"rmse_train\":torch.sqrt(torch.stack(self.losses[\"train_sqr_e\"]).mean()), # Raíz del error cuadrático medio de entrenamiento.\n",
    "                \"mae_train\":torch.stack(self.losses[\"train_avd_e\"]).mean(), # Valor absoluto medio de entrenamiento.\n",
    "                \"std_val\":std_val, # Desviación estándar de las pérdidas de validación.\n",
    "                \"mse_val\":torch.stack(self.losses[\"val_sqr_e\"]).mean(), # Error cuadrático medio de validación.\n",
    "                \"rmse_val\":torch.sqrt(torch.stack(self.losses[\"val_sqr_e\"]).mean()), # Raíz del error cuadrático medio de validación.\n",
    "                \"mae_val\":torch.stack(self.losses[\"val_avd_e\"]).mean(), # Valor absoluto medio de validación.\n",
    "            }\n",
    "\n",
    "            # Si hay pérdidas disponibles para el conjunto de prueba (test), realiza lo siguiente.\n",
    "            # add test loss if available\n",
    "            if len(self.losses[\"test\"]) > 0:\n",
    "                # Filtra y selecciona las pérdidas de prueba (test_avd_e) que no contienen valores NaN o infinitos.\n",
    "                valid_losses_test = [loss for loss in self.losses[\"test_avd_e\"] if not torch.isnan(loss).any()]\n",
    "                valid_losses_test = [loss for loss in valid_losses_test if not torch.isinf(loss).any() and torch.max(loss) < float(\"inf\")]\n",
    "                # Calcula la desviación estándar de las pérdidas de prueba.                \n",
    "                std_test = torch.std(torch.stack(valid_losses_test))\n",
    "                # Agrega métricas relacionadas con el conjunto de prueba al diccionario 'result_dict'.\n",
    "                result_dict[\"test_loss\"] = torch.stack(self.losses[\"test\"]).mean()\n",
    "                result_dict[\"std_test\"] = std_test\n",
    "                result_dict[\"mse_test\"] = torch.stack(self.losses[\"test_sqr_e\"]).mean()\n",
    "                result_dict[\"rmse_test\"] = torch.sqrt(torch.stack(self.losses[\"test_sqr_e\"]).mean())\n",
    "                result_dict[\"mae_test\"] = torch.stack(self.losses[\"test_avd_e\"]).mean()\n",
    "            # Si se disponen de pérdidas para predicciones ('train_y' y 'train_neg_dy') y derivadas ('train_neg_dy'), también las registra por separado.            \n",
    "            # if prediction and derivative are present, also log them separately\n",
    "            if len(self.losses[\"train_y\"]) > 0 and len(self.losses[\"train_neg_dy\"]) > 0:\n",
    "                result_dict[\"train_loss_y\"] = torch.stack(self.losses[\"train_y\"]).mean()\n",
    "                result_dict[\"train_loss_neg_dy\"] = torch.stack(\n",
    "                    self.losses[\"train_neg_dy\"]\n",
    "                ).mean()\n",
    "                result_dict[\"val_loss_y\"] = torch.stack(self.losses[\"val_y\"]).mean()\n",
    "                result_dict[\"val_loss_neg_dy\"] = torch.stack(self.losses[\"val_neg_dy\"]).mean()\n",
    "\n",
    "                # Si hay pérdidas disponibles para el conjunto de prueba, también las registra por separado.\n",
    "                if len(self.losses[\"test\"]) > 0:\n",
    "                    result_dict[\"test_loss_y\"] = torch.stack(\n",
    "                        self.losses[\"test_y\"]\n",
    "                    ).mean()\n",
    "                    result_dict[\"test_loss_neg_dy\"] = torch.stack(\n",
    "                        self.losses[\"test_neg_dy\"]\n",
    "                    ).mean()\n",
    "            # Registra las métricas en el registro y las sincroniza si se está utilizando distribución (sync_dist=True).\n",
    "            self.log_dict(result_dict, sync_dist=True)\n",
    "        # Restablece el diccionario de pérdidas para futuras épocas.\n",
    "        self._reset_losses_dict()\n",
    "\n",
    "    def _reset_losses_dict(self):\n",
    "        self.losses = {\n",
    "            \"train\": [],\n",
    "            \"val\": [],\n",
    "            \"train_sqr_e\": [],\n",
    "            \"train_avd_e\": [],\n",
    "            \"val_sqr_e\": [],\n",
    "            \"val_avd_e\": [],\n",
    "            \"test\": [],\n",
    "            \"test_sqr_e\": [],\n",
    "            \"test_avd_e\": [],\n",
    "            \"train_y\": [],\n",
    "            \"val_y\": [],\n",
    "            \"test_y\": [],\n",
    "            \"train_neg_dy\": [],\n",
    "            \"val_neg_dy\": [],\n",
    "            \"test_neg_dy\": [],\n",
    "        }\n",
    "\n",
    "    def _reset_ema_dict(self):\n",
    "        self.ema = {\"train_y\": None, \"val_y\": None, \"train_neg_dy\": None, \"val_neg_dy\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulos del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger, WandbLogger\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "import yaml\n",
    "#from torchmdnet.module import LNNP\n",
    "#from torchmdnet import datasets, priors, models\n",
    "#from torchmdnet.data import DataModule\n",
    "#from torchmdnet.models import output_modules\n",
    "#from torchmdnet.models.model import create_prior_models\n",
    "#from torchmdnet.models.utils import rbf_class_mapping, act_class_mapping, dtype_mapping\n",
    "#from torchmdnet.utils import LoadFromFile, LoadFromCheckpoint, save_argparse, number\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para tomar argumentos de la terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser(description='Training')\n",
    "    parser.add_argument('--load-model', action=LoadFromCheckpoint, help='Restart training using a model checkpoint')  # keep first\n",
    "    parser.add_argument('--conf', '-c', type=open, action=LoadFromFile, help='Configuration yaml file')  # keep second\n",
    "    parser.add_argument('--num-epochs', default=300, type=int, help='number of epochs')\n",
    "    parser.add_argument('--batch-size', default=32, type=int, help='batch size')\n",
    "    parser.add_argument('--inference-batch-size', default=None, type=int, help='Batchsize for validation and tests.')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--lr-patience', type=int, default=10, help='Patience for lr-schedule. Patience per eval-interval of validation')\n",
    "    parser.add_argument('--lr-metric', type=str, default='val_loss', choices=['train_loss', 'val_loss'], help='Metric to monitor when deciding whether to reduce learning rate')\n",
    "    parser.add_argument('--lr-min', type=float, default=1e-6, help='Minimum learning rate before early stop')\n",
    "    parser.add_argument('--lr-factor', type=float, default=0.8, help='Factor by which to multiply the learning rate when the metric stops improving')\n",
    "    parser.add_argument('--lr-warmup-steps', type=int, default=0, help='How many steps to warm-up over. Defaults to 0 for no warm-up')\n",
    "    parser.add_argument('--early-stopping-patience', type=int, default=30, help='Stop training after this many epochs without improvement')\n",
    "    parser.add_argument('--reset-trainer', type=bool, default=False, help='Reset training metrics (e.g. early stopping, lr) when loading a model checkpoint')\n",
    "    parser.add_argument('--weight-decay', type=float, default=0.0, help='Weight decay strength')\n",
    "    parser.add_argument('--ema-alpha-y', type=float, default=1.0, help='The amount of influence of new losses on the exponential moving average of y')\n",
    "    parser.add_argument('--ema-alpha-neg-dy', type=float, default=1.0, help='The amount of influence of new losses on the exponential moving average of dy')\n",
    "    parser.add_argument('--ngpus', type=int, default=-1, help='Number of GPUs, -1 use all available. Use CUDA_VISIBLE_DEVICES=1, to decide gpus')\n",
    "    parser.add_argument('--num-nodes', type=int, default=1, help='Number of nodes')\n",
    "    parser.add_argument('--precision', type=int, default=32, choices=[16, 32, 64], help='Floating point precision')\n",
    "    parser.add_argument('--log-dir', '-l', default='output', help='log file')\n",
    "    parser.add_argument('--splits', default=None, help='Npz with splits idx_train, idx_val, idx_test')\n",
    "    parser.add_argument('--train-size', type=number, default=None, help='Percentage/number of samples in training set (None to use all remaining samples)')\n",
    "    parser.add_argument('--val-size', type=number, default=0.05, help='Percentage/number of samples in validation set (None to use all remaining samples)')\n",
    "    parser.add_argument('--test-size', type=number, default=0.1, help='Percentage/number of samples in test set (None to use all remaining samples)')\n",
    "    parser.add_argument('--test-interval', type=int, default=10, help='Test interval, one test per n epochs (default: 10)')\n",
    "    parser.add_argument('--save-interval', type=int, default=10, help='Save interval, one save per n epochs (default: 10)')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    parser.add_argument('--num-workers', type=int, default=4, help='Number of workers for data prefetch')\n",
    "    parser.add_argument('--redirect', type=bool, default=False, help='Redirect stdout and stderr to log_dir/log')\n",
    "    parser.add_argument('--gradient-clipping', type=float, default=0.0, help='Gradient clipping norm')\n",
    "\n",
    "    # dataset specific\n",
    "    parser.add_argument('--dataset', default=None, type=str, help='Name of the torch_geometric dataset')\n",
    "    parser.add_argument('--dataset-root', default='input/data', type=str, help='Data storage directory (not used if dataset is \"CG\")')\n",
    "    parser.add_argument('--dataset-arg', default=None, type=str, help='Additional dataset arguments, e.g. target property for QM9 or molecule for MD17. Need to be specified in JSON format i.e. \\'{\"molecules\": \"aspirin,benzene\"}\\'')\n",
    "    parser.add_argument('--coord-files', default=None, type=str, help='Custom coordinate files glob')\n",
    "    parser.add_argument('--embed-files', default=None, type=str, help='Custom embedding files glob')\n",
    "    parser.add_argument('--energy-files', default=None, type=str, help='Custom energy files glob')\n",
    "    parser.add_argument('--force-files', default=None, type=str, help='Custom force files glob')\n",
    "    parser.add_argument('--y-weight', default=1.0, type=float, help='Weighting factor for y label in the loss function')\n",
    "    parser.add_argument('--neg-dy-weight', default=1.0, type=float, help='Weighting factor for neg_dy label in the loss function')\n",
    "\n",
    "    # model architecture\n",
    "    parser.add_argument('--model', type=str, default='graph-network', help='Which model to train')\n",
    "    parser.add_argument('--output-model', type=str, default='Scalar', help='The type of output model')\n",
    "    parser.add_argument('--prior-model', type=str, default=None, help='Which prior model to use')\n",
    "\n",
    "    # architectural args\n",
    "    parser.add_argument('--charge', type=bool, default=False, help='Model needs a total charge')\n",
    "    parser.add_argument('--spin', type=bool, default=False, help='Model needs a spin state')\n",
    "    parser.add_argument('--embedding-dimension', type=int, default=256, help='Embedding dimension')\n",
    "    parser.add_argument('--num-layers', type=int, default=6, help='Number of interaction layers in the model')\n",
    "    parser.add_argument('--num-rbf', type=int, default=64, help='Number of radial basis functions in model')\n",
    "    parser.add_argument('--activation', type=str, default='silu', choices=list(act_class_mapping.keys()), help='Activation function')\n",
    "    parser.add_argument('--rbf-type', type=str, default='expnorm', choices=list(rbf_class_mapping.keys()), help='Type of distance expansion')\n",
    "    parser.add_argument('--trainable-rbf', type=bool, default=False, help='If distance expansion functions should be trainable')\n",
    "    parser.add_argument('--neighbor-embedding', type=bool, default=False, help='If a neighbor embedding should be applied before interactions')\n",
    "    parser.add_argument('--aggr', type=str, default='add', help='Aggregation operation for CFConv filter output. Must be one of \\'add\\', \\'mean\\', or \\'max\\'')\n",
    "\n",
    "    # Transformer specific\n",
    "    parser.add_argument('--distance-influence', type=str, default='both', choices=['keys', 'values', 'both', 'none'], help='Where distance information is included inside the attention')\n",
    "    parser.add_argument('--attn-activation', default='silu', choices=list(act_class_mapping.keys()), help='Attention activation function')\n",
    "    parser.add_argument('--num-heads', type=int, default=8, help='Number of attention heads')\n",
    "    \n",
    "    # TensorNet specific\n",
    "    parser.add_argument('--equivariance-invariance-group', type=str, default='O(3)', help='Equivariance and invariance group of TensorNet')\n",
    "\n",
    "    # other args\n",
    "    parser.add_argument('--derivative', default=False, type=bool, help='If true, take the derivative of the prediction w.r.t coordinates')\n",
    "    parser.add_argument('--cutoff-lower', type=float, default=0.0, help='Lower cutoff in model')\n",
    "    parser.add_argument('--cutoff-upper', type=float, default=5.0, help='Upper cutoff in model')\n",
    "    parser.add_argument('--atom-filter', type=int, default=-1, help='Only sum over atoms with Z > atom_filter')\n",
    "    parser.add_argument('--max-z', type=int, default=100, help='Maximum atomic number that fits in the embedding matrix')\n",
    "    parser.add_argument('--max-num-neighbors', type=int, default=32, help='Maximum number of neighbors to consider in the network')\n",
    "    parser.add_argument('--standardize', type=bool, default=False, help='If true, multiply prediction by dataset std and add mean')\n",
    "    parser.add_argument('--reduce-op', type=str, default='add', choices=['add', 'mean'], help='Reduce operation to apply to atomic predictions')\n",
    "    parser.add_argument('--wandb-use', default=False, type=bool, help='Defines if wandb is used or not')\n",
    "    parser.add_argument('--wandb-name', default='training', type=str, help='Give a name to your wandb run')\n",
    "    parser.add_argument('--wandb-project', default='training_', type=str, help='Define what wandb Project to log to')\n",
    "    parser.add_argument('--wandb-resume-from-id', default=None, type=str, help='Resume a wandb run from a given run id. The id can be retrieved from the wandb dashboard')\n",
    "    parser.add_argument('--tensorboard-use', default=False, type=bool, help='Defines if tensor board is used or not')\n",
    "\n",
    "    # fmt: on\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Carga los valores desde un archivo YAML\n",
    "    with open('input/ET-QM9.yaml', 'r') as file:\n",
    "        yaml_data = yaml.safe_load(file)\n",
    "\n",
    "    # Asigna los valores del archivo YAML a los argumentos que coinciden\n",
    "    for key, value in yaml_data.items():\n",
    "        if hasattr(args, key):\n",
    "            setattr(args, key, value)\n",
    "\n",
    "    if args.redirect:\n",
    "        sys.stdout = open(os.path.join(args.log_dir, \"log\"), \"w\")\n",
    "        sys.stderr = sys.stdout\n",
    "        logging.getLogger(\"pytorch_lightning\").addHandler(\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        )\n",
    "\n",
    "    if args.inference_batch_size is None:\n",
    "        args.inference_batch_size = args.batch_size\n",
    "\n",
    "    save_argparse(args, os.path.join(args.log_dir, \"input.yaml\"), exclude=[\"conf\"])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]  # Esto elimina los argumentos adicionales de la celda de Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = get_args()\n",
    "pl.seed_everything(args.seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 110000, val 10000, test 10831\n"
     ]
    }
   ],
   "source": [
    "# initialize data module\n",
    "data = DataModule(args)\n",
    "data.prepare_data()\n",
    "data.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_models = create_prior_models(vars(args), data.dataset)\n",
    "args.prior_args = [p.get_init_args() for p in prior_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lightning module\n",
    "model = LNNP(args, prior_model=prior_models, mean=data.mean, std=data.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=args.log_dir,\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=10,  # -1 to save all\n",
    "    every_n_epochs=args.save_interval,\n",
    "    filename=\"{epoch}-{val_loss:.4f}-{test_loss:.4f}\",\n",
    ")\n",
    "early_stopping = EarlyStopping(\"val_loss\", patience=args.early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(args.log_dir, name=\"\", version=\"\")\n",
    "_logger = [csv_logger]\n",
    "if args.wandb_use:\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=args.wandb_project,\n",
    "        name=args.wandb_name,\n",
    "        save_dir=args.log_dir,\n",
    "        resume=\"must\" if args.wandb_resume_from_id is not None else None,\n",
    "        id=args.wandb_resume_from_id,\n",
    "    )\n",
    "    _logger.append(wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard_use:\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(\n",
    "        args.log_dir, name=\"tensorbord\", version=\"\", default_hp_metric=False\n",
    "    )\n",
    "    _logger.append(tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "#    strategy=DDPStrategy(find_unused_parameters=False),\n",
    "#    strategy='dp',\n",
    "    max_epochs=args.num_epochs,\n",
    "    gpus=args.ngpus,\n",
    "    num_nodes=args.num_nodes,\n",
    "    default_root_dir=args.log_dir,\n",
    "    auto_lr_find=False,\n",
    "    resume_from_checkpoint=None if args.reset_trainer else args.load_model,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    logger=_logger,\n",
    "    precision=args.precision,\n",
    "    gradient_clip_val=args.gradient_clipping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 110000, val 10000, test 10831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | TorchMD_Net | 6.9 M \n",
      "--------------------------------------\n",
      "6.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 M     Total params\n",
      "27.460    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 5/1798 [00:03<21:16,  1.40it/s, loss=5.22e+03, v_num=]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 33/1798 [00:12<10:44,  2.74it/s, loss=5.19e+03, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790><function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "self._shutdown_workers()      File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "\n",
      "self._shutdown_workers()  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    \n",
      "self._shutdown_workers()  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    \n",
      "if w.is_alive():  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "        if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError\n",
      ": can only test a child processAssertionError\n",
      "AssertionError: : can only test a child processcan only test a child process\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 40/1798 [1:00:49<44:33:34, 91.25s/it, loss=5.17e+03, v_num=]\n",
      "Epoch 0:   2%|▏         | 40/1798 [1:00:49<44:33:36, 91.25s/it, loss=5.17e+03, v_num=]\n",
      "Epoch 0:   2%|▏         | 40/1798 [1:00:50<44:33:37, 91.25s/it, loss=5.17e+03, v_num=]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    Exception ignored in: Exception ignored in: self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790><function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "\n",
      "      File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "Traceback (most recent call last):\n",
      "if w.is_alive():    \n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "self._shutdown_workers()      File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'      File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "\n",
      "    self._shutdown_workers()AssertionError\n",
      "if w.is_alive()::   File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "\n",
      "can only test a child process      File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "    AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "can only test a child processAssertionError\n",
      ": can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 35/1798 [00:13<11:44,  2.50it/s, loss=5.21e+03, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 40/1798 [1:00:51<44:34:50, 91.29s/it, loss=5.17e+03, v_num=]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "AssertionErrorcan only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 40/1798 [1:00:52<44:35:10, 91.30s/it, loss=5.17e+03, v_num=]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 36/1798 [00:15<12:20,  2.38it/s, loss=5.21e+03, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 40/1798 [1:00:52<44:35:43, 91.32s/it, loss=5.17e+03, v_num=]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb8f939f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/root/mambaforge/envs/torchmd-net/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 40/1798 [1:00:57<44:39:00, 91.43s/it, loss=5.17e+03, v_num=]\n",
      "Epoch 0:  17%|█▋        | 313/1798 [01:29<07:06,  3.49it/s, loss=165, v_num=]     "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test set after completing the fit\n",
    "model = LNNP.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "trainer = pl.Trainer(logger=_logger)\n",
    "trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución de la función principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "main()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmd-net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
